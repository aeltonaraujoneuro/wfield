{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wfield import *\n",
    "import tifffile\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.gridspec as gridspec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_scans(data_folder, keyword):\n",
    "    # Find folders containing the keyword\n",
    "    scan_folders = [folder for folder in os.listdir(data_folder) if os.path.isdir(os.path.join(data_folder, folder)) and keyword in folder]\n",
    "\n",
    "    # Print list of found folders\n",
    "    print(f\"Folders containing '{keyword}' keyword:\")\n",
    "    for i, folder in enumerate(scan_folders):\n",
    "        print(f\"{i + 1}. {folder}\")\n",
    "\n",
    "    # Prompt user to choose a folder\n",
    "    while True:\n",
    "        choice = input(\"Enter the number of the scan you want to choose: \")\n",
    "        if choice.isdigit() and 1 <= int(choice) <= len(scan_folders):\n",
    "            chosen_folder = scan_folders[int(choice) - 1]\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid input. Please enter a valid number.\")\n",
    "    print(\"Selected\",chosen_folder)\n",
    "    # Return the path to the chosen folder\n",
    "    return os.path.join(data_folder, chosen_folder), chosen_folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bins(localdisk):\n",
    "    # Find all .tif files in the directory\n",
    "    tif_files = [f for f in os.listdir(localdisk) if f.endswith('.tif')]\n",
    "    \n",
    "    # Find all .dat files in the directory\n",
    "    dat_files = glob(pjoin(localdisk,'*.dat'))\n",
    "    \n",
    "    if len(dat_files) > 1:\n",
    "        print(\"Multiple .dat files found:\")\n",
    "        for i, dat_file in enumerate(dat_files):\n",
    "            print(f\"{i + 1}. {os.path.basename(dat_file)}\")\n",
    "        \n",
    "        while True:\n",
    "            choice = input(\"Enter the number of the .dat file you want to choose: \")\n",
    "            if choice.isdigit() and 1 <= int(choice) <= len(dat_files):\n",
    "                dat_path = dat_files[int(choice) - 1]\n",
    "                break\n",
    "            else:\n",
    "                print(\"Invalid input. Please enter a valid number.\")\n",
    "    \n",
    "    elif len(dat_files) == 1:\n",
    "        dat_path = dat_files[0]\n",
    "    else:\n",
    "        print(\"Binaries file not found\")\n",
    "        print(\"-----------------------\")\n",
    "        print(\"Concatenating TIF files...\")\n",
    "        print(\"-----------------------\")\n",
    "        tif_data_list = []\n",
    "        for tif_file in sorted(tif_files):\n",
    "            tif_file_path = os.path.join(localdisk, tif_file)\n",
    "            tif_data = tifffile.imread(tif_file_path);\n",
    "            tif_data_list.append(tif_data)\n",
    "        concatenated_data = np.concatenate(tif_data_list, axis=0)\n",
    "        print(concatenated_data.shape)\n",
    "        # Splitting into violet and green channels\n",
    "        violet_channel = concatenated_data[::2]  # Every second frame starting from the first\n",
    "        green_channel = concatenated_data[1::2]  # Every second frame starting from the second\n",
    "\n",
    "        # Ensure both channels have the same length (in case the number of frames is odd)\n",
    "        if violet_channel.shape[0] > green_channel.shape[0]:\n",
    "            violet_channel = violet_channel[:-1]\n",
    "        h, w = green_channel.shape[1:]\n",
    "        merged_data = np.stack((violet_channel, green_channel), axis=1)\n",
    "        n_frames = merged_data.shape[0]\n",
    "        filename_parts = re.split(r'_|\\.', tif_files[0])\n",
    "        scan_info = '_'.join(filename_parts[0:4])  # Joining parts 1 to 4 with underscores\n",
    "        frame_shape = f\"{h}_{w}\"\n",
    "        filename = f\"{scan_info}_{frame_shape}_2_uint16.dat\"\n",
    "        print(f\"Saving file in {filename}\")\n",
    "        save_path = os.path.join(localdisk, filename)\n",
    "        merged_data.astype(np.uint16).tofile(save_path)\n",
    "        del merged_data\n",
    "        dat_path = dat_files[0]\n",
    "    \n",
    "    print(\"Binaries file found\")\n",
    "    print(\"Loading...\")\n",
    "    dat = mmap_dat(dat_path)\n",
    "    n_frames, h, w, n_channels = dat.shape\n",
    "    \n",
    "    dat = mmap_dat(dat_path, mode='r+', nframes=n_frames, shape=(2, h, w))\n",
    "    print(f'Loaded {n_frames} frames with {h} x {w} (height x width)')\n",
    "    print(\"Selected\", os.path.basename(dat_path))\n",
    "    return dat, n_frames, h, w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_video_from_array(array, filename_prefix, folder, num_channels):\n",
    "    n_frames, *shape = array.shape\n",
    "    height, width = shape[-2:]\n",
    "\n",
    "    # Create folder if it doesn't exist\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    if len(shape) == 2:  # Single channel\n",
    "        channels_to_save = 1\n",
    "    elif len(shape) == 3:  # Multiple channels\n",
    "        channels_to_save = min(num_channels, shape[0])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid array shape.\")\n",
    "\n",
    "    # Iterate over each channel\n",
    "    for channel_idx in range(channels_to_save):\n",
    "        # Create VideoWriter object\n",
    "        filename = os.path.join(folder, f\"{filename_prefix}_channel{channel_idx}.mp4\")\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Change codec to MP4V for MP4 format\n",
    "        out = cv2.VideoWriter(filename, fourcc, 25.0, (width, height))\n",
    "\n",
    "        # Iterate over each frame\n",
    "        for frame_idx in range(n_frames):\n",
    "            # Extract channel if multiple channels, else use array directly\n",
    "            channel_array = array[frame_idx, channel_idx] if len(shape) == 3 else array[frame_idx]\n",
    "\n",
    "            # Normalize pixel values to 0-255 range\n",
    "            channel_array = (channel_array - np.min(channel_array)) / (np.max(channel_array) - np.min(channel_array)) * 255\n",
    "\n",
    "            # Convert to uint8\n",
    "            frame = channel_array.astype(np.uint8)\n",
    "\n",
    "            # Apply colormap if needed (e.g., for grayscale images)\n",
    "            frame = cv2.applyColorMap(frame, cv2.COLORMAP_VIRIDIS)\n",
    "\n",
    "            # Write frame to video file\n",
    "            out.write(frame)\n",
    "\n",
    "        # Release VideoWriter object\n",
    "        out.release()\n",
    "\n",
    "    print(\"Videos saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def dFF_z(fluorescence_data, base_start, base_end):\n",
    "    \"\"\"\n",
    "    Calculate dF/F₀ and z-scores for fluorescence data.\n",
    "\n",
    "    Parameters:\n",
    "    - fluorescence_data (array-like): Fluorescence data.\n",
    "    - base_start (int): Start index of the baseline period.\n",
    "    - base_end (int): End index of the baseline period.\n",
    "\n",
    "    Returns:\n",
    "    - dF_over_F0 (DataFrame): DataFrame containing the dF/F₀ values.\n",
    "    - z_scores (DataFrame): DataFrame containing the z-scores.\n",
    "    \"\"\"\n",
    "    # Calculate F₀ (baseline fluorescence)\n",
    "    if base_end:\n",
    "        F0 = np.mean(fluorescence_data[base_start:base_end])\n",
    "        # print(f\"Using as baseline frames: [{base_start}, {base_end}]\")\n",
    "    else:\n",
    "        F0 = np.mean(fluorescence_data)\n",
    "        # print(\"Using as baseline scan average\")\n",
    "    \n",
    "    # Calculate dF (change in fluorescence)\n",
    "    dF = fluorescence_data - F0\n",
    "    \n",
    "    # Calculate dF/F₀\n",
    "    dF_over_F0 = dF / F0\n",
    "    \n",
    "    # Normalize dF/F₀ to 0-1 range\n",
    "    dF_over_F0_min = np.min(dF_over_F0)\n",
    "    dF_over_F0_max = np.max(dF_over_F0)\n",
    "    dF_over_F0_normalized = (dF_over_F0 - dF_over_F0_min) / (dF_over_F0_max - dF_over_F0_min)\n",
    "    \n",
    "    # Convert normalized dF/F₀ to percentage\n",
    "    dF_over_F0_percent = dF_over_F0_normalized * 100\n",
    "    \n",
    "    # Calculate z-scores for the normalized dF/F₀ values\n",
    "    z_scores = (dF_over_F0 - dF_over_F0.mean()) / np.std(dF_over_F0)\n",
    "    # z_scores = (dF_over_F0_normalized - np.mean(dF_over_F0_normalized)) / np.std(dF_over_F0_normalized)\n",
    "    \n",
    "    \n",
    "    return dF_over_F0, z_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_bpod(localdisk):\n",
    "    import scipy.io\n",
    "    mat_files = glob(pjoin(localdisk,'*.mat'))\n",
    "    if len(mat_files) > 1:\n",
    "        print(\"Multiple Bpod files found:\")\n",
    "        for i, mat_file in enumerate(mat_files):\n",
    "            print(f\"{i + 1}. {os.path.basename(mat_file)}\")\n",
    "        \n",
    "        while True:\n",
    "            choice = input(\"Enter the number of the .dat file you want to choose: \")\n",
    "            if choice.isdigit() and 1 <= int(choice) <= len(mat_files):\n",
    "                mat_path = mat_files[int(choice) - 1]\n",
    "                break\n",
    "            else:\n",
    "                print(\"Invalid input. Please enter a valid number.\")\n",
    "    elif len(mat_files) == 1:\n",
    "        mat_path = mat_files[0]\n",
    "        print(\"Bpod file found\")\n",
    "        print(\"Loading...\")\n",
    "    else:\n",
    "        print(\"Bpod file not found\")\n",
    "        print(\"-----------------------\")\n",
    "    \n",
    "    try:\n",
    "        bpod_data = scipy.io.loadmat(mat_path)\n",
    "        print(\"Selected\", os.path.basename(mat_path))\n",
    "    except:\n",
    "        print(\"Failed\")\n",
    "    return bpod_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders containing 'AA' keyword:\n",
      "1. Habituation_AA_WEZ-8950_2024-04-16_scan9FNN1Y64_sess9FNN0LFP\n",
      "2. Habituation_AA_WEZ-8948_2024-04-17_scan9FNNP1N7_sess9FNNO3Z1\n",
      "3. Oddball_AA_ROS-1706_2024-03-12_scan9FN2BCOS_sess9FN2ANVG\n",
      "4. AA_ROS-1688_2024_01_27_scan000WQU9_sess000EAEIO\n",
      "5. Habituation_AA_WEZ-8950_2024-04-18_scan9FNO8CLT_sess9FNO8CLT\n",
      "6. Habituation_AA_WEZ-8950_2024-04-17_scan9FNNO3Z1_sess9FNNO3Z1\n",
      "7. AA_ROS-1706_2024-03-12_scan9FN2ANVG_sess9FN2ANVG\n",
      "8. Habituation_AA_WEZ-8950_2024-04-16_scan9FNN1YXK_sess9FNN1YXK\n",
      "9. Habituation_AA_WEZ-8948_2024-04-18_scan9FNO99ZE_sess9FNO8CLT\n",
      "10. AA_ROS-1688_2024_01_27_scan000EAEIO_sess000EAEIO\n",
      "11. Habituation_AA_WEZ-8950_2024-04-16_scan9FNN2FX7_sess9FNN1YXK\n",
      "12. Habituation_AA_WEZ-8948_2024-04-16_scan9FNN1M1R_sess9FNN0LFP\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number of the scan you want to choose:  10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected AA_ROS-1688_2024_01_27_scan000EAEIO_sess000EAEIO\n"
     ]
    }
   ],
   "source": [
    "data_folder = r'/datajoint-data/data/aeltona/'\n",
    "# tif_file_path = pjoin(data_folder, 'scan9FN2ANVG_Oddball_AA_ROS-1706_2025_MMStack_Default.ome.tif')\n",
    "# localdisk = r'C:\\datatemp'\n",
    "localdisk, scan_idx = list_scans(data_folder,\"AA\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple .dat files found:\n",
      "1. scan000EAEIO_AA_ROS-1688_2024_647_730_2_uint16.dat\n",
      "2. motioncorrected_scan000EAEIO_AA_ROS-1688_2024_647_730_2_uint16.dat\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number of the .dat file you want to choose:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binaries file found\n",
      "Loading...\n",
      "Loaded 3047 frames with 647 x 730 (height x width)\n",
      "Selected motioncorrected_scan000EAEIO_AA_ROS-1688_2024_647_730_2_uint16.dat\n"
     ]
    }
   ],
   "source": [
    "dat,n_frames, h, w = find_bins(localdisk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # save_video_from_array(violet_channel, f\"{scan_idx}_violet_uncorrected\",localdisk,1)\n",
    "# save_video_from_array(green_channel, f\"{scan_idx}_green_uncorrected\",localdisk,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = pjoin(localdisk,'motion_corrected.dat')\n",
    "out = np.empty_like(dat)\n",
    "\n",
    "(yshifts,xshifts),rot = motion_correct(dat, out = out, chunksize=512,\n",
    "                                     apply_shifts=True)\n",
    "del dat # close and finish writing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tif_files = [f for f in os.listdir(localdisk) if f.endswith('.tif')]\n",
    "\n",
    "filename_parts = re.split(r'_|\\.', tif_files[0])\n",
    "scan_info = '_'.join(filename_parts[0:4])  # Joining parts 1 to 4 with underscores\n",
    "frame_shape = f\"{h}_{w}\"\n",
    "filename = f\"motioncorrected_{scan_info}_{frame_shape}_2_uint16.dat\"\n",
    "print(f\"Saving file in {filename}\")\n",
    "save_path = os.path.join(localdisk, filename)\n",
    "out.astype(np.uint16).tofile(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save the shifts\n",
    "shifts = np.rec.array([yshifts,xshifts],dtype=[('y','float32'),('x','float32')])\n",
    "np.save(pjoin(localdisk,'motion_correction_shifts.npy'),shifts)\n",
    "# np.save(pjoin(data_folder,'motion_correction_shifts.npy'),shifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "plt.matplotlib.style.use('ggplot')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# localdisk = '/mnt/dual/temp_folder/CSP23_20200226' # this should be an SSD or a fast drive\n",
    "\n",
    "shifts = np.load(pjoin(localdisk,'motion_correction_shifts.npy'))\n",
    "\n",
    "plot_summary_motion_correction(shifts,localdisk);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial Onsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Aux file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = r'/datajoint-data/data/aeltona/'\n",
    "# tif_file_path = pjoin(data_folder, 'scan9FN2ANVG_Oddball_AA_ROS-1706_2025_MMStack_Default.ome.tif')\n",
    "# localdisk = r'C:\\datatemp'\n",
    "\n",
    "localdisk, scan_idx = list_scans(data_folder,\"AA\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/datajoint-data/data/aeltona/AA_ROS-1688_2024_01_27_scan000EAEIO_sess000EAEIO/scan000EAEIO_AA_ROS-1688_2024_01_27_Preconditioning.h5'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5_path = glob(pjoin(localdisk,'*.h5'))[0]\n",
    "h5_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Open the HDF5 file\n",
    "with h5py.File(h5_path, 'r') as f:\n",
    "    # Access specific group\n",
    "    # print(f.keys())\n",
    "    sweep_data_key = list(f.keys())[1]\n",
    "    sweep_data = f[sweep_data_key]\n",
    "    header = f['header']\n",
    "    # List all keys within the group\n",
    "    # print(f\"Keys in {sweep_data_key}: %s\" % sweep_data.keys())\n",
    "    # print(\"Keys in header: %s\" % header.keys())\n",
    "    \n",
    "    \n",
    "    AIChannelNames = header['AIChannelNames'][:]\n",
    "    YLimitsPerAIChannel = header['YLimitsPerAIChannel'][:]\n",
    "    AIChannelNames = [name.decode('utf-8') for name in AIChannelNames]\n",
    "    SampleRate = header['AcquisitionSampleRate'][:]\n",
    "    analogData = sweep_data['analogScans'][:]\n",
    "    # # Check if 'data' exists within the group\n",
    "    # if 'analogScans' in sweep_data:\n",
    "    #     data = sweep_data['analogScans'][:]  # accessing array dataset\n",
    "    #     print(\"Data from sweep_0001:\", data)\n",
    "    # else:\n",
    "    #     print(\"Dataset 'data' does not exist in sweep_0001.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert analogData to DataFrame\n",
    "df = pd.DataFrame(analogData.T, columns=AIChannelNames)\n",
    "sampling_rate_hz = SampleRate[0,0]\n",
    "df['time_seconds'] = df.index / sampling_rate_hz\n",
    "# Print DataFrame\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['camera_trigger', 'blue_470nm', 'violet_405nm', 'HiFi_module', 'Reward',\n",
       "       'SoftCode', 'Lick Spout', 'Rotary Encoder', 'time_seconds'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(df['Rotary Encoder']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tbd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3047, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Oddball = False\n",
    "SP = True\n",
    "if Oddball:\n",
    "    norm_stim_id = df['Stim_ID']/3000\n",
    "    norm_stim_id = norm_stim_id.round(0)\n",
    "    norm_stim_on = (df['Stim_ON'] - df['Stim_ON'].min()) / (df['Stim_ON'].max() - df['Stim_ON'].min())\n",
    "    norm_stim_on = norm_stim_on.round(0)\n",
    "    df['norm_stim_on'] = norm_stim_on\n",
    "    df['norm_stim_id'] = norm_stim_id\n",
    "elif SP:\n",
    "    print('tbd')\n",
    "\n",
    "norm_blue_on = (df['blue_470nm'] - df['blue_470nm'].min()) / (df['blue_470nm'].max() - df['blue_470nm'].min())\n",
    "norm_blue_on = norm_blue_on.round(0)\n",
    "df['norm_blue_on'] = norm_blue_on\n",
    "\n",
    "blue_df = df[df['norm_blue_on'] == 1]\n",
    "\n",
    "points_per_frame = len(blue_df) // n_frames\n",
    "blue_df_reshaped = pd.DataFrame()\n",
    "for col in blue_df.columns:\n",
    "    reshaped_col = blue_df[col].values[:points_per_frame * n_frames].reshape(n_frames, -1)\n",
    "    blue_df_reshaped[col] = reshaped_col[:, 0]\n",
    "data_df = blue_df_reshaped\n",
    "data_df.shape\n",
    "# norm_blue_on.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Bpod Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bpod file found\n",
      "Loading...\n",
      "Selected ROS-1688_SensoryPreconditioning_20240127_175031.mat\n"
     ]
    }
   ],
   "source": [
    "bpod_data = list_bpod(localdisk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_onsets = bpod_data['SessionData'][0, 0]['TrialStartTimestamp'][0]\n",
    "trial_types = bpod_data['SessionData'][0, 0]['TrialTypes'][0]\n",
    "n_trials = bpod_data['SessionData'][0, 0]['nTrials'][0][0]\n",
    "\n",
    "stim_1 = bpod_data['SessionData'][0, 0]['RawEvents']['Trial'][0,0][0,0]['States'][0,0]['Stimulus4On'][0,0][0,:]\n",
    "\n",
    "\n",
    "reward =  bpod_data['SessionData'][0, 0]['RawEvents']['Trial'][0,0][0,0]['States'][0,0]['Reward'][0,0][0,:]\n",
    "odd_trials = np.where(trial_types==2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_stim_bpod = stim_1[0]\n",
    "\n",
    "# Find the first stimulus time in data_df\n",
    "first_stim_df_index = data_df[data_df['norm_stim_on'] == 1].index[0]\n",
    "first_stim_df_time = data_df.loc[first_stim_df_index, 'time_seconds']\n",
    "\n",
    "# Calculate the time delay between data_df and Bpod\n",
    "time_delay = first_stim_df_time - (first_stim_bpod + trial_onsets[0])\n",
    "\n",
    "# Adjust the Bpod trial onsets by the time delay\n",
    "adjusted_trial_onsets = trial_onsets + time_delay\n",
    "# Create a new column in data_df for trial onsets, initialized to 0\n",
    "data_df['trial_onsets'] = 0\n",
    "# Mark the frames (rows) corresponding to the adjusted trial onsets\n",
    "for onset in adjusted_trial_onsets:\n",
    "    # Find the closest frame (row) in data_df corresponding to each adjusted trial onset\n",
    "    closest_idx = (np.abs(data_df['time_seconds'] - onset)).idxmin()\n",
    "    data_df.at[closest_idx, 'trial_onsets'] = 1\n",
    "\n",
    "adjusted_odd_stim = []\n",
    "for trial_num in odd_trials[0]:\n",
    "    oddball = bpod_data['SessionData'][0, 0]['RawEvents']['Trial'][0,0][0,trial_num]['States'][0,0]['Stimulus1On'][0,0][0,:]\n",
    "    \n",
    "    adjusted = oddball + trial_onsets[trial_num] +  time_delay# + time_delay\n",
    "    # print(oddball,trial_onsets[trial_num],adjusted)\n",
    "    adjusted_odd_stim.append(adjusted)\n",
    "\n",
    "adjusted_reward_onsets = []\n",
    "for trial_num in range(0, n_trials):\n",
    "    reward =  bpod_data['SessionData'][0, 0]['RawEvents']['Trial'][0,0][0,trial_num]['States'][0,0]['Reward'][0,0][0,:]\n",
    "    adjusted = reward + trial_onsets[trial_num] +  time_delay\n",
    "    adjusted_reward_onsets.append(adjusted)\n",
    "\n",
    "\n",
    "# adjusted_odd_stim\n",
    "data_df['oddball_onsets'] = 0\n",
    "\n",
    "for onset_offset in adjusted_odd_stim:\n",
    "    onset, offset = onset_offset\n",
    "    # Find indices where the time is within the onset and offset range\n",
    "    indices_in_range = data_df[(data_df['time_seconds'] >= onset) & (data_df['time_seconds'] <= offset)].index\n",
    "    # Set the corresponding rows in 'oddball_onsets' to 1\n",
    "    data_df.loc[indices_in_range, 'oddball_onsets'] = 1\n",
    "\n",
    "data_df['reward_onsets'] = 0\n",
    "\n",
    "for onset_offset in adjusted_reward_onsets:\n",
    "    onset, offset = onset_offset\n",
    "    indices_in_range = data_df[(data_df['time_seconds'] >= onset) & (data_df['time_seconds'] <= offset)].index\n",
    "    data_df.loc[indices_in_range, 'reward_onsets'] = 1\n",
    "\n",
    "start = 0\n",
    "end = 1500\n",
    "\n",
    "\n",
    "plt.plot(data_df['time_seconds'][start:end],data_df['norm_stim_on'][start:end],color = 'black')\n",
    "plt.plot(data_df['time_seconds'][start:end],data_df['oddball_onsets'][start:end],color = 'r')\n",
    "plt.plot(data_df['time_seconds'][start:end],data_df['trial_onsets'][start:end],color = 'b')\n",
    "plt.plot(data_df['time_seconds'][start:end],data_df['reward_onsets'][start:end],color = 'purple')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "end = 5000\n",
    "\n",
    "\n",
    "plt.plot(data_df['time_seconds'][start:end],data_df['norm_stim_on'][start:end],color = 'black')\n",
    "plt.plot(data_df['time_seconds'][start:end],data_df['oddball_onsets'][start:end],color = 'r')\n",
    "plt.plot(data_df['time_seconds'][start:end],data_df['trial_onsets'][start:end],color = 'b')\n",
    "plt.plot(data_df['time_seconds'][start:end],data_df['reward_onsets'][start:end],color = 'purple')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match Dorsal Cortex to the Allen Atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import show\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frame averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found frames average\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    frames_average = np.load(pjoin(localdisk,'frames_average.npy'))\n",
    "    print(\"Found frames average\")\n",
    "except:\n",
    "    print(\"Frames average not found, averaging...\")\n",
    "    nbaseline_frames = 10\n",
    "    trial_onsets = data_df['trial_onsets']\n",
    "    frames_average_trials = frames_average_for_trials(dat,\n",
    "                                                        trial_onsets,\n",
    "                                                        nbaseline_frames)\n",
    "\n",
    "  \n",
    "        \n",
    "    np.save(pjoin(localdisk,'frames_average.npy'), frames_average)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m frame_averages \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m trial_onsets \u001b[38;5;241m=\u001b[39m data_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrial_onsets\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m on \u001b[38;5;129;01min\u001b[39;00m tqdm(trial_onsets):\n\u001b[1;32m      4\u001b[0m     frame_averages\u001b[38;5;241m.\u001b[39mappend(dat[on:on\u001b[38;5;241m+\u001b[39mnbaseline_frames]\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_df' is not defined"
     ]
    }
   ],
   "source": [
    "frame_averages = []\n",
    "trial_onsets = data_df['trial_onsets']\n",
    "for on in tqdm(trial_onsets):\n",
    "    frame_averages.append(dat[on:on+nbaseline_frames].mean(axis=0))\n",
    "frame_average = np.stack(frames_averages)\n",
    "    \n",
    "np.save(pjoin(localdisk,'frames_average.npy'), frames_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbaseline_frames = 10\n",
    "trial_onsets = data_df['trial_onsets']\n",
    "frames_average_trials = frames_average_for_trials(dat,trial_onsets,nbaseline_frames)        \n",
    "np.save(pjoin(localdisk,'frames_average.npy'), frames_average_trials.mean(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "resolution = 0.015 # mm per pixel\n",
    "image = frames_average[0] # load a frame from the references \n",
    "bregma_offset = (np.array(image.shape[::-1])/2).astype('int')\n",
    "\n",
    "ccf_regions,proj,brain_outline = allen_load_reference('dorsal_cortex')\n",
    "\n",
    "# Get or load landmarks, these are for the allen dorsal cortex dataset\n",
    "landmarks = {'x': [-1.95, 0, 1.95, 0],\n",
    "             'y': [-3.45, -3.45, -3.45, 3.2],\n",
    "             'name': ['OB_left', 'OB_center', 'OB_right', 'RSP_base'],\n",
    "             'color': ['#fc9d03', '#0367fc', '#fc9d03', '#fc4103']}\n",
    "landmarks = pd.DataFrame(landmarks)\n",
    "\n",
    "# Load holoviews with the bokeh backend\n",
    "import holoviews as hv\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks = {'x': [-1.95, 0, 1.95, 0],\n",
    "             'y': [-3.45, -3.45, -3.45, 3.2],\n",
    "             'name': ['OB_left', 'OB_center', 'OB_right', 'RSP_base'],\n",
    "             'color': ['#fc9d03', '#0367fc', '#fc9d03', '#fc4103']}\n",
    "\n",
    "wid,lmark_wid = hv_adjust_reference_landmarks(landmarks,ccf_regions)\n",
    "wid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the new landmarks (save these somewhere)\n",
    "# WARNING: This only works if you change something (edit the table)\n",
    "landmarks = pd.DataFrame(lmark_wid.data)[['x','y','name','color']] # landmarks in allen_coords\n",
    "\n",
    "save_allen_landmarks(landmarks,\n",
    "                     resolution = resolution,\n",
    "                     bregma_offset = bregma_offset)\n",
    "landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmarks['resolution']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wfield import *\n",
    "\n",
    "# load data and references\n",
    "# localdisk = '/home/joao/dual/temp_folder/'\n",
    "frames_average = np.load(pjoin(localdisk,'frames_average.npy'))\n",
    "image = frames_average[0]\n",
    "lmarks = load_allen_landmarks(None)\n",
    "landmarks = lmarks['landmarks']\n",
    "if 'landmarks_match' in lmarks.keys():\n",
    "    landmarks_match = lmarks['landmarks_match']\n",
    "else:\n",
    "    landmarks_match = None\n",
    "# bregma_offset = lmarks['bregma_offset']\n",
    "bregma_offset = [300,320]\n",
    "resolution = lmarks['resolution']\n",
    "\n",
    "# The following line lets you plot previous landmarks\n",
    "# how to load a transform\n",
    "# landmarks_match = landmarks_match[['x','y','name','color']]\n",
    "\n",
    "\n",
    "wid,lmark_wid,landmarks_im = hv_adjust_image_landmarks(image,landmarks,\n",
    "                                                       landmarks_match = landmarks_match,\n",
    "                                                       bregma_offset = bregma_offset,\n",
    "                                                       resolution = resolution)\n",
    "wid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the similarity transform and plot the result\n",
    "landmarks_match = pd.DataFrame(lmark_wid.data)\n",
    "M = allen_transform_from_landmarks(landmarks_im,landmarks_match)\n",
    "overlay = hv_show_transformed_overlay(frames_average[0], M, ccf_regions,bregma_offset=bregma_offset,resolution = resolution)\n",
    "show(hv.render(overlay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the transform and the reference points\n",
    "save_allen_landmarks(landmarks,\n",
    "                    #  filename = pjoin(localdisk,'ccf_transform.json'),\n",
    "                     filename = pjoin(localdisk,'ccf_transform_landmarks.json'),\n",
    "                     landmarks_match=landmarks_match,\n",
    "                     transform=M,\n",
    "                     resolution = resolution,\n",
    "                     bregma_offset = bregma_offset)\n",
    "print('''Transform:\n",
    "    - scale {0}\n",
    "    - translation {1}\n",
    "    - rotation {2}'''.format(M.scale,M.translation,np.rad2deg(M.rotation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an allen reference with holoviews\n",
    "allen_regions = hv_plot_allen_regions(ccf_regions).options(width = w, height=h)\n",
    "show(hv.render(allen_regions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with matplotlib (example)\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=[8,8])\n",
    "h,w = image.shape\n",
    "plt.imshow(im_apply_transform(image,M),\n",
    "           cmap='gray')\n",
    "for i,r in landmarks_im.iterrows():\n",
    "    m = landmarks_match.loc[i]\n",
    "    plt.plot(m.x,m.y,'g+',ms=10)\n",
    "    plt.plot(r.x,r.y,'rx',ms=20)\n",
    "    plt.text(r.x,r.y,r['name'],color='y',fontsize=12,va='bottom',ha='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wfield import atlas_from_landmarks_file\n",
    "from wfield import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmarks_path = pjoin(localdisk,'ccf_transform_landmarks.json')\n",
    "atlas, areanames, brain_mask = atlas_from_landmarks_file(lmarks_path,dims = [h,w],do_transform=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames_average = np.load(pjoin(localdisk,'frames_average.npy'))\n",
    "frames_average = frames_average_trials\n",
    "\n",
    "ch1data = dat[:,1,:,:]\n",
    "mask  = get_std_mask(ch1data,threshold=45)\n",
    "U,SVT = approximate_svd(dat, frames_average, mask = atlas, k = 200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "from wfield import nb_play_movie\n",
    "from wfield import SVDStack\n",
    "%matplotlib widget\n",
    "stack = SVDStack(U,SVT)\n",
    "plt.figure()\n",
    "nb_play_movie(stack,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(pjoin(localdisk,'U.npy'),U)\n",
    "np.save(pjoin(localdisk,'SVT.npy'),SVT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # filter and interpolate\n",
    "# t = np.arange(SVT_470.shape[1]*2) # interpolate the violet\n",
    "# from scipy.interpolate import interp1d\n",
    "# SVT_405 = interp1d(t[1::2],SVT_405,axis=1,\n",
    "#                    fill_value='extrapolate')(t[0::2])\n",
    "# freq_highpass = 0.1\n",
    "# fs = 10.\n",
    "# SVT_470 = highpass(SVT_470,w = freq_highpass, fs = fs).astype(np.float32)\n",
    "# SVT_405 = highpass(SVT_405,w = freq_highpass, fs = fs).astype(np.float32)\n",
    "\n",
    "# # ref_folder = '/home/aeltona/wfield/references'\n",
    "# # Apply the transform, prepare for plotting\n",
    "# lmarks = load_allen_landmarks(pjoin(localdisk,'ccf_transform_landmarks.json'))\n",
    "# ccf_regions,proj,brain_outline = allen_load_reference('dorsal_cortex')\n",
    "# bout = brain_outline/lmarks['resolution'] + np.array(lmarks['bregma_offset'])\n",
    "\n",
    "# mask = contour_to_mask(*bout.T,dims = U.shape[:-1])\n",
    "\n",
    "# if 'dims' in dir():\n",
    "#     U = U.reshape(dims)\n",
    "\n",
    "# # warp and mask U, first make boarders zero\n",
    "# from wfield.imutils import mask_to_3d\n",
    "# U[:,0,:] = 0\n",
    "# U[0,:,:] = 0\n",
    "# U[-1,:,:] = 0\n",
    "# U[:,-1,:] = 0\n",
    "# U = np.stack(runpar(im_apply_transform,\n",
    "#                     U.transpose([2,0,1]),\n",
    "#                     M = lmarks['transform']))\n",
    "# U[mask_to_3d(mask,U.shape)==0] = np.nan\n",
    "# U = U.transpose([1,2,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hemodynamics correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = localdisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U = np.load(pjoin(folder,'U.npy'))\n",
    "# SVT = np.load(pjoin(folder,'SVT.npy'))\n",
    "SVT_470 = SVT[:,::2]\n",
    "SVT_405 = SVT[:,1::2]\n",
    "# SVT_corr = np.load(pjoin(folder,'SVTcorr.npy'))\n",
    "\n",
    "# T = np.load(pjoin(folder,'T.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 5\n",
    "\n",
    "output_folder = localdisk\n",
    "\n",
    "tstart = time.time()\n",
    "\n",
    "SVT_470 = SVT[:,0::2]\n",
    "t = np.arange(SVT.shape[1]) # interpolate the violet\n",
    "from scipy.interpolate import interp1d\n",
    "SVT_405 = interp1d(t[1::2],SVT[:,1::2],axis=1,\n",
    "                    fill_value='extrapolate')(t[0::2])\n",
    "SVTcorr, rcoeffs, T = hemodynamic_correction(U, \n",
    "                                             SVT_470, \n",
    "                                             SVT_405, \n",
    "                                             fs=fs,\n",
    "                                             freq_lowpass=2)  \n",
    "\n",
    "print('Done hemodynamic correction in {0} s '.format(time.time()-tstart))\n",
    "\n",
    "np.save(pjoin(localdisk,'rcoeffs.npy'),rcoeffs)\n",
    "np.save(pjoin(localdisk,'T.npy'),T)\n",
    "np.save(pjoin(localdisk,'SVTcorr.npy'),SVTcorr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: # don't crash while plotting\n",
    "    import pylab as plt\n",
    "    plt.matplotlib.style.use('ggplot')\n",
    "    from wfield import  plot_summary_hemodynamics_dual_colors\n",
    "    frame_rate = 5.\n",
    "\n",
    "    plot_summary_hemodynamics_dual_colors(rcoeffs,\n",
    "                                            SVT_470,\n",
    "                                            SVT_405,\n",
    "                                            U,\n",
    "                                            T,\n",
    "                                            frame_rate=frame_rate,\n",
    "                                            duration = 6,\n",
    "                                            outputdir = output_folder);\n",
    "except Exception as err:\n",
    "    print('There was an issue plotting.')\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(usvt.mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(vio.var());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(gre.var());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_video_from_array(usvt, f\"U_SVDStack\",localdisk,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Warping Stack ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "U = np.load(pjoin(localdisk,'U_atlas.npy'))\n",
    "SVT = np.load(pjoin(localdisk,'SVTcorr.npy'))\n",
    "stack = SVDStack(U,SVT)\n",
    "lmarks = load_allen_landmarks(pjoin(localdisk,'ccf_transform_landmarks.json'))\n",
    "\n",
    "ccf_regions_reference,proj,brain_outline = allen_load_reference('dorsal_cortex')\n",
    "# the reference is in allen CCF space and needs to be converted\n",
    "# this converts to warped image space (accounting for the transformation)\n",
    "ccf_regions = allen_transform_regions(None,ccf_regions_reference,\n",
    "                                      resolution = lmarks['resolution'],\n",
    "                                        bregma_offset = lmarks['bregma_offset'])\n",
    "atlas, areanames, brain_mask = atlas_from_landmarks_file(pjoin(localdisk,'ccf_transform_landmarks.json')) # this loads the atlas in transformed coords\n",
    "\n",
    "# this does the transform (warps the original images)\n",
    "stack.set_warped(1, M = lmarks['transform']) # this warps the spatial components in the stack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this converts the reference to image space (unwarped)\n",
    "ref_folder = '/home/aeltona/wfield/references'\n",
    "atlas_im, areanames, brain_mask = atlas_from_landmarks_file(pjoin(ref_folder,'dorsal_cortex_landmarks.json'),do_transform = True) # this loads the untransformed atlas\n",
    "ccf_regions_im = allen_transform_regions(lmarks['transform'],ccf_regions_reference,\n",
    "                                        resolution = lmarks['resolution'],\n",
    "                                        bregma_offset = lmarks['bregma_offset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets compare the warped with the unwarped average activity in an area\n",
    "# area 33 is VISp\n",
    "area = 33\n",
    "stack.set_warped(True) # once this is done once the transform is set and you can alternate between the 2 modes.\n",
    "warped = stack.get_timecourse(np.where(atlas == area)).mean(axis = 0)\n",
    "stack.set_warped(False)\n",
    "unwarped = stack.get_timecourse(np.where(atlas_im == area)).mean(axis = 0)\n",
    "\n",
    "fig = plt.figure(figsize = [7,10])\n",
    "fig.add_subplot(2,1,1)\n",
    "plt.plot(unwarped,'k',lw = .5,label = 'unwarped average')\n",
    "plt.plot(warped,'r',lw = .5,label = 'warped average')\n",
    "plt.legend();\n",
    "plt.xlim([2000,2500])\n",
    "# plt.ylim([-0.04,0.06]);\n",
    "\n",
    "area = -33 # plot the other side\n",
    "stack.set_warped(True) \n",
    "warped = stack.get_timecourse(np.where(atlas == area)).mean(axis = 0)\n",
    "stack.set_warped(False)\n",
    "unwarped = stack.get_timecourse(np.where(atlas_im == area)).mean(axis = 0)\n",
    "\n",
    "fig.add_subplot(2,1,2)\n",
    "plt.plot(unwarped,'k',lw = .5,label = 'unwarped average')\n",
    "plt.plot(warped,'r',lw = .5,label = 'warped average')\n",
    "plt.legend()\n",
    "plt.xlim([2000,2500])\n",
    "\n",
    "# plt.xlim([20000,20500])\n",
    "# plt.ylim([-0.04,0.06]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 20 spatial components of the transformed dataset and the raw dataset\n",
    "fig = plt.figure(figsize = [15,10])\n",
    "ncomponents = 20\n",
    "for icomponent in range(ncomponents):\n",
    "    fig.add_subplot(5,4,icomponent+1)\n",
    "    plt.imshow(np.concatenate([stack.originalU[:,:,icomponent],\n",
    "                               stack.U_warped[:,:,icomponent]],axis = 1),\n",
    "               clim=[-0.01,0.01],cmap='Spectral_r')\n",
    "    # plot the regions overlayed on the raw images\n",
    "    for i,r in ccf_regions_im.iterrows():\n",
    "        plt.plot(r['left_x'],r['left_y'],'k',lw=0.3)\n",
    "    # plot the raw reference because the images were converted\n",
    "    for i,r in ccf_regions.iterrows():\n",
    "        plt.plot(np.array(r['left_x'])+stack.U.shape[1],r['left_y'],'k',lw=0.3)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 20 random frames of the transformed dataset and the raw dataset \n",
    "(sanity check: what does the transform do to the images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the 20 frames of the transformed dataset and the raw dataset\n",
    "fig = plt.figure(figsize = [15,10])\n",
    "for i,iframe in enumerate(np.random.choice(np.arange(0,stack.shape[0]),20)):\n",
    "    fig.add_subplot(5,4,i+1)\n",
    "    plt.imshow(np.concatenate([reconstruct(stack.originalU,stack.SVT[:,iframe]),\n",
    "                               reconstruct(stack.U_warped,stack.SVT[:,iframe])],axis = 1),\n",
    "               clim=[-0.03,0.03],cmap='Spectral_r')\n",
    "    # plot the regions overlayed on the raw images\n",
    "    for i,r in ccf_regions_im.iterrows():\n",
    "        plt.plot(r['left_x'],r['left_y'],'k',lw=0.3)\n",
    "    # plot the raw reference because the images were converted\n",
    "    for i,r in ccf_regions.iterrows():\n",
    "        plt.plot(np.array(r['left_x'])+stack.U.shape[1],r['left_y'],'k',lw=0.3)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highlight the differences in the atlas and ROIs in both warped and raw spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the overlap for the same region in the warped versus unwarped atlases \n",
    "plt.figure(figsize = [5,5])\n",
    "area1 = 4\n",
    "area2 = 33\n",
    "reg = np.zeros([*atlas.shape,3])\n",
    "reg[:,:,0] = np.array((atlas == area1) | (atlas == area2))*255\n",
    "reg[:,:,1] = np.array((atlas_im == area1) | (atlas_im == area2))*255\n",
    "plt.imshow(reg.astype('uint8'))\n",
    "\n",
    "for i,r in ccf_regions.iterrows():\n",
    "    for side in ['right']:\n",
    "        plt.plot(np.array(r[side+'_x']),r[side +'_y'],'r',lw=1)\n",
    "for i,r in ccf_regions_im.iterrows():\n",
    "    for side in ['right']:\n",
    "        plt.plot(np.array(r[side+'_x']),r[side +'_y'],'g',lw=1)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the converted stack on the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play a movie of the stack with the regions overlayed\n",
    "fig = plt.figure()\n",
    "# This is for stack.set_warped(True)\n",
    "stack.set_warped(True)\n",
    "for i,r in ccf_regions.iterrows():\n",
    "    for side in ['left','right']:\n",
    "        plt.plot(np.array(r[side+'_x']),r[side +'_y'],'k',lw=0.3);\n",
    "plt.axis('off')\n",
    "nb_play_movie(stack,cmap = 'Spectral_r',clim=[-0.04,0.04]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate ΔF/F and Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find intervals where norm_stim_on is 0\n",
    "zero_intervals = []\n",
    "start = None\n",
    "for idx, row in data_df.iterrows():\n",
    "    if row['norm_stim_on'] == 0:\n",
    "        if start is None:\n",
    "            start = idx\n",
    "    else:\n",
    "        if start is not None:\n",
    "            zero_intervals.append((start, idx - 1))\n",
    "            start = None\n",
    "if start is not None:\n",
    "    zero_intervals.append((start, len(data_df) - 1))\n",
    "\n",
    "# Sort intervals by length in descending order\n",
    "sorted_intervals = sorted(zero_intervals, key=lambda x: x[1] - x[0], reverse=True)\n",
    "\n",
    "print(\"Sorted Intervals...\")\n",
    "# for interval in sorted_intervals:\n",
    "#     print(interval)\n",
    "\n",
    "# Find intervals between trial_onsets and next stimulus\n",
    "# trial_intervals =pend((trial_onset, data_df.loc[idx + 1, 'frame']))\n",
    "\n",
    "# print(\"\\nIntervals between trial_onsets and next stimulus:\")\n",
    "# for interval in trial_intervals:\n",
    "#     print(interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0,5):\n",
    "    print(sorted_intervals[i])\n",
    "# print(base_start, base_end )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_start, base_end = sorted_intervals[3]\n",
    "\n",
    "U = np.load(pjoin(localdisk,'U.npy'))\n",
    "SVT = np.load(pjoin(localdisk,'SVTcorr.npy'))\n",
    "\n",
    "lmarks = load_allen_landmarks(pjoin(localdisk,'ccf_transform_landmarks.json'))\n",
    "atlas, areanames, brain_mask = atlas_from_landmarks_file(pjoin(localdisk,'ccf_transform_landmarks.json')) # this loads the atlas in transformed coords\n",
    "\n",
    "stack = SVDStack(U,SVT)\n",
    "stack.set_warped(True)\n",
    "for area_number in range(0,len(areanames)):\n",
    "    area_name = str(areanames[area_number][1])\n",
    "    area = areanames[area_number][0]\n",
    "    # print(area,area_name)\n",
    "    try:\n",
    "        data = stack.get_timecourse(np.where(atlas == area)).mean(axis = 0)\n",
    "        dFF, zscore = dFF_z(data,base_start,base_end);\n",
    "        # dFF, zscore = dFF_z(data)\n",
    "        data_df[f'{area_name}_dFF'] = dFF\n",
    "        data_df[f'{area_name}_z'] = zscore\n",
    "        print(area_name, 'loaded')\n",
    "    except Exception as e:\n",
    "        \n",
    "        print(f\"An error occurred: {e} \",area_name, 'failed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the intervals of stimuli\n",
    "one_intervals = []\n",
    "start = None\n",
    "for idx, row in data_df.iterrows():\n",
    "    if row['norm_stim_on'] == 1:\n",
    "        if start is None:\n",
    "            start = idx\n",
    "    else:\n",
    "        if start is not None:\n",
    "            one_intervals.append((start, idx - 1))\n",
    "            start = None\n",
    "if start is not None:\n",
    "    one_intervals.append((start, len(data_df) - 1))\n",
    "\n",
    "# Sort intervals by the order of appearance\n",
    "stim_intervals = sorted(one_intervals, key=lambda x: x[0])\n",
    "\n",
    "# Create a DataFrame for the stimulus intervals\n",
    "stimulus_data = {\n",
    "    'stim_num': [],\n",
    "    'onset': [],\n",
    "    'offset': [],\n",
    "    'stim_id': []\n",
    "}\n",
    "\n",
    "for stim_num, (onset, offset) in enumerate(stim_intervals):\n",
    "    stim_id = data_df.loc[onset, 'norm_stim_id']\n",
    "    stimulus_data['stim_num'].append(stim_num+1)\n",
    "    stimulus_data['onset'].append(onset)\n",
    "    stimulus_data['offset'].append(offset)\n",
    "    stimulus_data['stim_id'].append(stim_id)\n",
    "\n",
    "stim_df = pd.DataFrame(stimulus_data)\n",
    "last_stim_num = stim_df['stim_num'].iloc[-1] // 4\n",
    "trial_num = [i // 4 + 1 for i in range(4 * last_stim_num)]\n",
    "stim_df['trial_num'] = trial_num\n",
    "# Print the resulting DataFrame\n",
    "\n",
    "odd_trials_array = odd_trials[0]  # Extracting the array from the tuple\n",
    "true_odd_trials = [trial + 1 for trial in odd_trials_array]\n",
    "stim_df.loc[(stim_df['stim_id'] == 9) & (stim_df['trial_num'].isin(true_odd_trials)), 'stim_id'] = 10\n",
    "stim_df['duration'] = stim_df['offset'] - stim_df['onset']\n",
    "mean_dur = stim_df.groupby('stim_id')['duration'].mean()\n",
    "sem_dur = stim_df.groupby('stim_id')['duration'].sem()\n",
    "stim_df['mean_dur'] = stim_df['stim_id'].map(mean_dur)\n",
    "stim_df['sem_dur'] = stim_df['stim_id'].map(sem_dur)\n",
    "display(stim_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select region and data type\n",
    "region = 'ACAd'\n",
    "data_type = 'z'\n",
    "\n",
    "time_window_before = 10  # 10 frames before stimulus onset\n",
    "time_window_after = 20  # 20 frames after stimulus onset\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=len(sides), figsize=(21, 7))\n",
    "sides = ['left', 'right']\n",
    "for i, side in enumerate(sides):\n",
    "    column_name = f'{region}_{side}_{data_type}'\n",
    "    \n",
    "    # Filter dataframe for the selected region and data type\n",
    "    region_data = data_df[[column_name, 'time_seconds']]\n",
    "    \n",
    "    # Initialize lists for peri-stimulus data\n",
    "    peri_stim_data_expected = []\n",
    "    peri_stim_data_oddball = []\n",
    "    offsets_expected = []\n",
    "    offsets_oddball = []\n",
    "    \n",
    "    # Extract indices of stimulus onsets and offsets from stim_df\n",
    "    exp_onsets = stim_df.loc[stim_df['stim_id'] == 9, 'onset'].tolist()\n",
    "    odd_onsets = stim_df.loc[stim_df['stim_id'] == 10, 'onset'].tolist()\n",
    "    exp_offsets = stim_df.loc[stim_df['stim_id'] == 9, 'offset'].tolist()\n",
    "    odd_offsets = stim_df.loc[stim_df['stim_id'] == 10, 'offset'].tolist()\n",
    "    exp_mean_dur = stim_df.loc[stim_df['stim_id'] == 9, 'mean_dur'].tolist()\n",
    "    odd_mean_dur = stim_df.loc[stim_df['stim_id'] == 10, 'mean_dur'].tolist()\n",
    "    exp_sem_dur = stim_df.loc[stim_df['stim_id'] == 9, 'sem_dur'].tolist()\n",
    "    odd_sem_dur = stim_df.loc[stim_df['stim_id'] == 10, 'sem_dur'].tolist()\n",
    "    \n",
    "    # Extract peri-stimulus data for expected stimuli\n",
    "    for onset, offset, mean_dur, sem_dur in zip(exp_onsets, exp_offsets, exp_mean_dur, exp_sem_dur):\n",
    "        start_idx = onset - time_window_before\n",
    "        end_idx = onset + time_window_after\n",
    "        if start_idx >= 0 and end_idx < len(region_data):\n",
    "            peri_stim_data_expected.append(region_data.iloc[start_idx:end_idx][column_name].values)\n",
    "            offsets_expected.append(offset - onset)\n",
    "    \n",
    "    # Extract peri-stimulus data for oddball stimuli\n",
    "    for onset, offset, mean_dur, sem_dur in zip(odd_onsets, odd_offsets, odd_mean_dur, odd_sem_dur):\n",
    "        start_idx = onset - time_window_before\n",
    "        end_idx = onset + time_window_after\n",
    "        if start_idx >= 0 and end_idx < len(region_data):\n",
    "            peri_stim_data_oddball.append(region_data.iloc[start_idx:end_idx][column_name].values)\n",
    "            offsets_oddball.append(offset - onset)\n",
    "\n",
    "    # Determine the maximum length of the peri-stimulus data\n",
    "    max_length = max(max(len(x) for x in peri_stim_data_expected), max(len(x) for x in peri_stim_data_oddball))\n",
    "    \n",
    "    # Pad or truncate the data to ensure consistent length\n",
    "    peri_stim_data_expected = [np.pad(x, (0, max_length - len(x)), 'constant', constant_values=np.nan) if len(x) < max_length else x[:max_length] for x in peri_stim_data_expected]\n",
    "    peri_stim_data_oddball = [np.pad(x, (0, max_length - len(x)), 'constant', constant_values=np.nan) if len(x) < max_length else x[:max_length] for x in peri_stim_data_oddball]\n",
    "\n",
    "    # Calculate mean activity for expected and oddball stimuli\n",
    "    mean_activity_expected = np.nanmean(peri_stim_data_expected, axis=0)\n",
    "    sem_activity_expected = np.nanstd(peri_stim_data_expected, axis=0) / np.sqrt(np.sum(~np.isnan(peri_stim_data_expected), axis=0))\n",
    "    mean_activity_oddball = np.nanmean(peri_stim_data_oddball, axis=0)\n",
    "    sem_activity_oddball = np.nanstd(peri_stim_data_oddball, axis=0) / np.sqrt(np.sum(~np.isnan(peri_stim_data_oddball), axis=0))\n",
    "\n",
    "    ax = axs[i]\n",
    "    \n",
    "    # Plot mean activity for expected stimuli\n",
    "    ax.plot(np.arange(-time_window_before, time_window_after), mean_activity_expected, label=f'{region} {side.capitalize()} - Expected', color='blue')\n",
    "    ax.fill_between(np.arange(-time_window_before, time_window_after), mean_activity_expected - sem_activity_expected, mean_activity_expected + sem_activity_expected, color='blue', alpha=0.3)\n",
    "    \n",
    "    # Plot mean activity for oddball stimuli\n",
    "    ax.plot(np.arange(-time_window_before, time_window_after), mean_activity_oddball, label=f'{region} {side.capitalize()} - Oddball', color='red')\n",
    "    ax.fill_between(np.arange(-time_window_before, time_window_after), mean_activity_oddball - sem_activity_oddball, mean_activity_oddball + sem_activity_oddball, color='red', alpha=0.3)\n",
    "    \n",
    "    # Plot stimulus onset and offset\n",
    "    ax.axvline(x=0, linestyle='--', color='gray', label='Stimulus Onset')  # Stimulus onset\n",
    "    ax.axvline(x=np.mean(exp_mean_dur), linestyle='--', color='blue', label='Expected Stimulus Offset')  # Average offset for expected stimuli\n",
    "    ax.axvline(x=np.mean(odd_mean_dur), linestyle='--', color='red', label='Oddball Stimulus Offset')  # Average offset for oddball stimuli\n",
    "    \n",
    "    # Shade stimulus period\n",
    "    ax.axvspan(0, np.mean(exp_mean_dur), alpha=0.3, color='orange')  # Stimulus period for expected stimuli\n",
    "    ax.axvspan(0, np.mean(odd_mean_dur), alpha=0.3, color='pink')  # Stimulus period for oddball stimuli\n",
    "\n",
    "    # Shade SEM for offsets\n",
    "    ax.axvspan(exp_mean_dur[0] - exp_sem_dur[0], exp_mean_dur[0] + exp_sem_dur[0], alpha=0.1, color='blue')\n",
    "    ax.axvspan(odd_mean_dur[0] - odd_sem_dur[0], odd_mean_dur[0] + odd_sem_dur[0], alpha=0.1, color='red')\n",
    "    \n",
    "    ax.set_title(f'{region} {side}')\n",
    "    ax.set_xlabel('Time (indices) relative to stimulus onset')\n",
    "    ax.set_ylabel('Activity (z-score)')\n",
    "    ax.legend();\n",
    "    ax.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(pjoin(localdisk, f'PSTH/{region}_PSTH.png'))\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over areanames to find existing columns\n",
    "for area_number in range(len(areanames)):\n",
    "    area_name = str(areanames[area_number][1])\n",
    "    column_name = f'{area_name}_{data}'\n",
    "    if column_name in data_df.columns:\n",
    "        column_data = data_df[column_name].values\n",
    "        if not np.isnan(column_data).all():  # Check if the entire column is NaN\n",
    "            columns_to_plot.append(column_name)\n",
    "        else:\n",
    "            excluded_columns.append(column_name) # This will print the first part of the name\n",
    "# Use a set to store unique base region names\n",
    "unique_regions = set()\n",
    "\n",
    "for region in columns_to_plot:\n",
    "    base_region_name = region.split('_')[0]\n",
    "    unique_regions.add(base_region_name)\n",
    "\n",
    "# Convert the set to a list if needed\n",
    "unique_regions = list(unique_regions)\n",
    "\n",
    "before = 10\n",
    "after = 20\n",
    "data_type = 'z'\n",
    "# Print unique base region names\n",
    "for region in unique_regions:\n",
    "    plot_PSTH(data_df, stim_df, region, before, after,data_type);\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_PSTH(data_df, stim_df, region, time_window_before, time_window_after,data_type):\n",
    "    sides = ['left', 'right']\n",
    "    # data_type = 'zscore'  # Assuming 'zscore' is the data type, adjust if necessary\n",
    "    print(f\"PSTH for {region} in progress...\")\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=len(sides), figsize=(21, 7))\n",
    "    \n",
    "    for i, side in enumerate(sides):\n",
    "        column_name = f'{region}_{side}_{data_type}'\n",
    "        \n",
    "        # Filter dataframe for the selected region and data type\n",
    "        region_data = data_df[[column_name, 'time_seconds']]\n",
    "        \n",
    "        # Initialize lists for peri-stimulus data\n",
    "        peri_stim_data_expected = []\n",
    "        peri_stim_data_oddball = []\n",
    "        offsets_expected = []\n",
    "        offsets_oddball = []\n",
    "        \n",
    "        # Extract indices of stimulus onsets and offsets from stim_df\n",
    "        exp_onsets = stim_df.loc[stim_df['stim_id'] == 9, 'onset'].tolist()\n",
    "        odd_onsets = stim_df.loc[stim_df['stim_id'] == 10, 'onset'].tolist()\n",
    "        exp_offsets = stim_df.loc[stim_df['stim_id'] == 9, 'offset'].tolist()\n",
    "        odd_offsets = stim_df.loc[stim_df['stim_id'] == 10, 'offset'].tolist()\n",
    "        exp_mean_dur = stim_df.loc[stim_df['stim_id'] == 9, 'mean_dur'].tolist()\n",
    "        odd_mean_dur = stim_df.loc[stim_df['stim_id'] == 10, 'mean_dur'].tolist()\n",
    "        exp_sem_dur = stim_df.loc[stim_df['stim_id'] == 9, 'sem_dur'].tolist()\n",
    "        odd_sem_dur = stim_df.loc[stim_df['stim_id'] == 10, 'sem_dur'].tolist()\n",
    "        \n",
    "        # Extract peri-stimulus data for expected stimuli\n",
    "        for onset, offset, mean_dur, sem_dur in zip(exp_onsets, exp_offsets, exp_mean_dur, exp_sem_dur):\n",
    "            start_idx = onset - time_window_before\n",
    "            end_idx = onset + time_window_after\n",
    "            if start_idx >= 0 and end_idx < len(region_data):\n",
    "                peri_stim_data_expected.append(region_data.iloc[start_idx:end_idx][column_name].values)\n",
    "                offsets_expected.append(offset - onset)\n",
    "        \n",
    "        # Extract peri-stimulus data for oddball stimuli\n",
    "        for onset, offset, mean_dur, sem_dur in zip(odd_onsets, odd_offsets, odd_mean_dur, odd_sem_dur):\n",
    "            start_idx = onset - time_window_before\n",
    "            end_idx = onset + time_window_after\n",
    "            if start_idx >= 0 and end_idx < len(region_data):\n",
    "                peri_stim_data_oddball.append(region_data.iloc[start_idx:end_idx][column_name].values)\n",
    "                offsets_oddball.append(offset - onset)\n",
    "\n",
    "        # Determine the maximum length of the peri-stimulus data\n",
    "        max_length = max(max(len(x) for x in peri_stim_data_expected), max(len(x) for x in peri_stim_data_oddball))\n",
    "        \n",
    "        # Pad or truncate the data to ensure consistent length\n",
    "        peri_stim_data_expected = [np.pad(x, (0, max_length - len(x)), 'constant', constant_values=np.nan) if len(x) < max_length else x[:max_length] for x in peri_stim_data_expected]\n",
    "        peri_stim_data_oddball = [np.pad(x, (0, max_length - len(x)), 'constant', constant_values=np.nan) if len(x) < max_length else x[:max_length] for x in peri_stim_data_oddball]\n",
    "\n",
    "        # Calculate mean activity for expected and oddball stimuli\n",
    "        mean_activity_expected = np.nanmean(peri_stim_data_expected, axis=0)\n",
    "        sem_activity_expected = np.nanstd(peri_stim_data_expected, axis=0) / np.sqrt(np.sum(~np.isnan(peri_stim_data_expected), axis=0))\n",
    "        mean_activity_oddball = np.nanmean(peri_stim_data_oddball, axis=0)\n",
    "        sem_activity_oddball = np.nanstd(peri_stim_data_oddball, axis=0) / np.sqrt(np.sum(~np.isnan(peri_stim_data_oddball), axis=0))\n",
    "\n",
    "        ax = axs[i]\n",
    "        \n",
    "        # Plot mean activity for expected stimuli\n",
    "        ax.plot(np.arange(-time_window_before, time_window_after), mean_activity_expected, label=f'{region} {side.capitalize()} - Expected', color='blue')\n",
    "        ax.fill_between(np.arange(-time_window_before, time_window_after), mean_activity_expected - sem_activity_expected, mean_activity_expected + sem_activity_expected, color='blue', alpha=0.3)\n",
    "        \n",
    "        # Plot mean activity for oddball stimuli\n",
    "        ax.plot(np.arange(-time_window_before, time_window_after), mean_activity_oddball, label=f'{region} {side.capitalize()} - Oddball', color='red')\n",
    "        ax.fill_between(np.arange(-time_window_before, time_window_after), mean_activity_oddball - sem_activity_oddball, mean_activity_oddball + sem_activity_oddball, color='red', alpha=0.3)\n",
    "        \n",
    "        # Plot stimulus onset and offset\n",
    "        ax.axvline(x=0, linestyle='--', color='gray', label='Stimulus Onset')  # Stimulus onset\n",
    "        ax.axvline(x=np.mean(exp_mean_dur), linestyle='--', color='blue', label='Expected Stimulus Offset')  # Average offset for expected stimuli\n",
    "        ax.axvline(x=np.mean(odd_mean_dur), linestyle='--', color='red', label='Oddball Stimulus Offset')  # Average offset for oddball stimuli\n",
    "        \n",
    "        # Shade stimulus period\n",
    "        ax.axvspan(0, np.mean(exp_mean_dur), alpha=0.3, color='orange')  # Stimulus period for expected stimuli\n",
    "        ax.axvspan(0, np.mean(odd_mean_dur), alpha=0.3, color='pink')  # Stimulus period for oddball stimuli\n",
    "\n",
    "        # Shade SEM for offsets\n",
    "        ax.axvspan(exp_mean_dur[0] - exp_sem_dur[0], exp_mean_dur[0] + exp_sem_dur[0], alpha=0.1, color='blue')\n",
    "        ax.axvspan(odd_mean_dur[0] - odd_sem_dur[0], odd_mean_dur[0] + odd_sem_dur[0], alpha=0.1, color='red')\n",
    "        \n",
    "        ax.set_title(f'{region} {side}')\n",
    "        ax.set_xlabel('Time (indices) relative to stimulus onset')\n",
    "        ax.set_ylabel('Activity (z-score)')\n",
    "        ax.legend()\n",
    "        ax.grid(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(pjoin(localdisk, f'PSTH/{region}_PSTH.png'))\n",
    "    print(f\"PSTH for {region} saved...\")\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define start and end points for the data slice\n",
    "# start, end = onset_intervals[-1]  \n",
    "\n",
    "# start -= 25\n",
    "# end += 50\n",
    "\n",
    "start, end = 0,data_df.shape[0]\n",
    "# Initialize lists to store columns and corresponding data\n",
    "columns_to_plot = []\n",
    "data_to_plot = []\n",
    "excluded_columns = []\n",
    "\n",
    "# Specify the data type ('dFF' or 'z')\n",
    "data = 'z'\n",
    "\n",
    "# Iterate over areanames to find existing columns\n",
    "for area_number in range(len(areanames)):\n",
    "    area_name = str(areanames[area_number][1])\n",
    "    column_name = f'{area_name}_{data}'\n",
    "    if column_name in data_df.columns:\n",
    "        column_data = data_df[column_name].values\n",
    "        if not np.isnan(column_data).all():  # Check if the entire column is NaN\n",
    "            columns_to_plot.append(column_name)\n",
    "            data_to_plot.append(column_data[start:end])  # Slice data to the specified range\n",
    "        else:\n",
    "            excluded_columns.append(column_name)\n",
    "\n",
    "# Check if any columns exist\n",
    "if len(columns_to_plot) == 0:\n",
    "    print(\"No columns found in the DataFrame.\")\n",
    "else:\n",
    "    # Create a gridspec with a color bar\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    gs = gridspec.GridSpec(3, 2, width_ratios=[15, 1], height_ratios=[1, 1, len(columns_to_plot)], hspace=0.05, wspace=0.05)\n",
    "    \n",
    "    # Plot norm_stim_on as bars on the top left subplot\n",
    "    ax1 = fig.add_subplot(gs[1, 0])\n",
    "    norm_stim_on_values = data_df['norm_stim_on'].values[start:end]\n",
    "    ax1.bar(data_df['time_seconds'].values[start:end], norm_stim_on_values, color='black', width=1)\n",
    "    ax1.set_xlim([data_df['time_seconds'].values[start], data_df['time_seconds'].values[end-1]])\n",
    "    ax1.axis('off')  # Turn off the axis\n",
    "\n",
    "    # Plot rotary encoder data\n",
    "    ax3 = fig.add_subplot(gs[0, 0])\n",
    "    encoder_values = data_df['Rotary Encoder'].values[start:end]\n",
    "    ax3.plot(data_df['time_seconds'].values[start:end], encoder_values, color='black')\n",
    "    ax3.set_xlim([data_df['time_seconds'].values[start], data_df['time_seconds'].values[end-1]])\n",
    "    ax3.axis('off')\n",
    "    \n",
    "    # Plot oddball_onsets and reward_onsets as bars\n",
    "    oddball_values = data_df['oddball_onsets'].values[start:end]\n",
    "    ax1.bar(data_df['time_seconds'].values[start:end], oddball_values, color='red', width=1)\n",
    "\n",
    "    reward_values = data_df['reward_onsets'].values[start:end]\n",
    "    ax1.bar(data_df['time_seconds'].values[start:end], reward_values, color='purple', width=1)\n",
    "\n",
    "    # Plot the heatmap on the bottom left subplot\n",
    "    ax2 = fig.add_subplot(gs[2, 0], sharex=ax1)\n",
    "    cax = ax2.imshow(data_to_plot, aspect='auto', cmap='bwr', interpolation='nearest', extent=[data_df['time_seconds'].values[start], data_df['time_seconds'].values[end-1], 0, len(columns_to_plot)])\n",
    "    ax2.set_xlabel('Time(s)')\n",
    "    ax2.set_ylabel('Areas')\n",
    "\n",
    "    # Set x-axis ticks for the heatmap\n",
    "    time_seconds = data_df['time_seconds'].values[start:end]\n",
    "    ax2.set_xticks(time_seconds[::int(len(time_seconds)/10)])  # Adjust the step for better labeling\n",
    "    ax2.set_xticklabels(time_seconds[::int(len(time_seconds)/10)], rotation=90)\n",
    "    \n",
    "    # Set y-axis ticks for the heatmap\n",
    "    ax2.set_yticks(range(len(columns_to_plot)))\n",
    "    ax2.set_yticklabels(columns_to_plot)\n",
    "    \n",
    "    # Add a color bar in the right subplot\n",
    "    cbar_ax = fig.add_subplot(gs[:, 1])\n",
    "    cbar = fig.colorbar(cax, cax=cbar_ax)\n",
    "    cbar.set_label(f'{data}')\n",
    "    ax2.grid(False)\n",
    "    # plt.show()\n",
    "# plt.savefig(pjoin(localdisk, '')\n",
    "# plt.savefig(pjoin(localdisk, 'whole_session.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tif_files = [f for f in os.listdir(localdisk) if f.endswith('.tif')]\n",
    "\n",
    "filename_parts = re.split(r'_|\\.', tif_files[0])\n",
    "scan_info = '_'.join(filename_parts[0:4])\n",
    "\n",
    "csv_path = pjoin(localdisk, scan_info+'.csv')\n",
    "print(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# garb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select region and data type\n",
    "region = 'ACAd'\n",
    "data_type = 'z'\n",
    "sides = ['left', 'right']\n",
    "time_window_before = 5  # 5 seconds before stimulus onset\n",
    "time_window_after = 10  # 10 seconds after stimulus offset\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=len(sides), figsize=(15, 5))\n",
    "\n",
    "for i, side in enumerate(sides):\n",
    "    column_name = f'{region}_{side}_{data_type}'\n",
    "    \n",
    "    # Filter dataframe for the selected region and data type\n",
    "    region_data = data_df[[column_name, 'norm_stim_on', 'oddball_onsets', 'norm_stim_id', 'time_seconds']]\n",
    "    \n",
    "    # Initialize lists for peri-stimulus data\n",
    "    peri_stim_data_expected = []\n",
    "    peri_stim_data_oddball = []\n",
    "    offsets_expected = []\n",
    "    offsets_oddball = []\n",
    "    \n",
    "    stim_onsets = region_data.index[region_data['norm_stim_on'] == 1].tolist()\n",
    "    oddball_onsets = region_data.index[region_data['oddball_onsets'] == 1].tolist()\n",
    "    expected_stimuli_indices = [idx for idx in stim_onsets if region_data.at[idx, 'norm_stim_id'] == 9 and idx not in oddball_onsets]\n",
    "\n",
    "    # Extract peri-stimulus data for expected stimuli\n",
    "    for onset in expected_stimuli_indices:\n",
    "        offset = region_data.index[(region_data.index > onset) & (region_data['norm_stim_on'] == 0)].tolist()\n",
    "        if offset:\n",
    "            offset = offset[0]\n",
    "            offsets_expected.append(offset - onset)\n",
    "            start_idx = onset - time_window_before\n",
    "            end_idx = offset + time_window_after\n",
    "            if start_idx >= 0 and end_idx < len(region_data):\n",
    "                peri_stim_data_expected.append(region_data.iloc[start_idx:end_idx][column_name].values)\n",
    "\n",
    "    # Extract peri-stimulus data for oddball stimuli\n",
    "    for onset in oddball_onsets:\n",
    "        offset = region_data.index[(region_data.index > onset) & (region_data['norm_stim_on'] == 0)].tolist()\n",
    "        if offset:\n",
    "            offset = offset[0]\n",
    "            offsets_oddball.append(offset - onset)\n",
    "            start_idx = onset - time_window_before\n",
    "            end_idx = offset + time_window_after\n",
    "            if start_idx >= 0 and end_idx < len(region_data):\n",
    "                peri_stim_data_oddball.append(region_data.iloc[start_idx:end_idx][column_name].values)\n",
    "\n",
    "    # Determine the maximum length of the peri-stimulus data\n",
    "    max_length = max(max(len(x) for x in peri_stim_data_expected), max(len(x) for x in peri_stim_data_oddball))\n",
    "\n",
    "    # Pad or truncate the data to ensure consistent length\n",
    "    peri_stim_data_expected = [np.pad(x, (0, max_length - len(x)), 'constant', constant_values=np.nan) if len(x) < max_length else x[:max_length] for x in peri_stim_data_expected]\n",
    "    peri_stim_data_oddball = [np.pad(x, (0, max_length - len(x)), 'constant', constant_values=np.nan) if len(x) < max_length else x[:max_length] for x in peri_stim_data_oddball]\n",
    "\n",
    "    # Calculate mean and SEM for expected stimuli\n",
    "    peri_stim_data_expected = np.array(peri_stim_data_expected)\n",
    "    mean_activity_expected = np.nanmean(peri_stim_data_expected, axis=0)\n",
    "    sem_activity_expected = np.nanstd(peri_stim_data_expected, axis=0) / np.sqrt(np.sum(~np.isnan(peri_stim_data_expected), axis=0))\n",
    "    \n",
    "    # Calculate mean and SEM for oddball stimuli\n",
    "    peri_stim_data_oddball = np.array(peri_stim_data_oddball)\n",
    "    mean_activity_oddball = np.nanmean(peri_stim_data_oddball, axis=0)\n",
    "    sem_activity_oddball = np.nanstd(peri_stim_data_oddball, axis=0) / np.sqrt(np.sum(~np.isnan(peri_stim_data_oddball), axis=0))\n",
    "    \n",
    "    time_vector = np.linspace(-time_window_before, time_window_after, max_length)  # Adjust time vector for the peri-stimulus window\n",
    "\n",
    "    # Calculate average and SEM for offsets\n",
    "    mean_offset_expected = np.mean(offsets_expected)\n",
    "    sem_offset_expected = np.std(offsets_expected) / np.sqrt(len(offsets_expected))\n",
    "    mean_offset_oddball = np.mean(offsets_oddball)\n",
    "    sem_offset_oddball = np.std(offsets_oddball) / np.sqrt(len(offsets_oddball))\n",
    "\n",
    "    ax = axs[i]\n",
    "    \n",
    "    # Plot mean activity for expected stimuli\n",
    "    ax.plot(time_vector, mean_activity_expected, label=f'{region} {side.capitalize()} - Expected', color='blue')\n",
    "    ax.fill_between(time_vector, mean_activity_expected - sem_activity_expected, mean_activity_expected + sem_activity_expected, color='blue', alpha=0.3)\n",
    "    \n",
    "    # Plot mean activity for oddball stimuli\n",
    "    ax.plot(time_vector, mean_activity_oddball, label=f'{region} {side.capitalize()} - Oddball', color='red')\n",
    "    ax.fill_between(time_vector, mean_activity_oddball - sem_activity_oddball, mean_activity_oddball + sem_activity_oddball, color='red', alpha=0.3)\n",
    "    \n",
    "    # Plot stimulus onset and offset\n",
    "    ax.axvline(x=0, linestyle='--', color='gray', label='Stimulus Onset')  # Stimulus onset\n",
    "    ax.axvline(x=mean_offset_expected, linestyle='--', color='blue', label='Expected Stimulus Offset')  # Average offset for expected stimuli\n",
    "    ax.axvline(x=mean_offset_oddball, linestyle='--', color='red', label='Oddball Stimulus Offset')  # Average offset for oddball stimuli\n",
    "    \n",
    "    # Shade stimulus period\n",
    "    ax.axvspan(0, mean_offset_expected, alpha=0.3, color='orange')\n",
    "    \n",
    "    # Shade SEM for offsets\n",
    "    ax.axvspan(mean_offset_expected - sem_offset_expected, mean_offset_expected + sem_offset_expected, alpha=0.1, color='blue')\n",
    "    ax.axvspan(mean_offset_oddball - sem_offset_oddball, mean_offset_oddball + sem_offset_oddball, alpha=0.1, color='red')\n",
    "    \n",
    "    ax.set_title(f'Peri-stimulus Activity - {side.capitalize()} Side')\n",
    "    ax.set_xlabel('Time (s) relative to stimulus onset')\n",
    "    ax.set_ylabel('Activity (z-score)')\n",
    "    ax.legend('_')\n",
    "    ax.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
