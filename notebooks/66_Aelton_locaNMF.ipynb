{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_scans(data_folder, keyword):\n",
    "    # Find folders containing the keyword\n",
    "    scan_folders = [folder for folder in os.listdir(data_folder) if os.path.isdir(os.path.join(data_folder, folder)) and keyword in folder]\n",
    "\n",
    "    # Print list of found folders\n",
    "    print(\"Folders containing '{}' keyword:\")\n",
    "    for i, folder in enumerate(scan_folders):\n",
    "        print(f\"{i + 1}. {folder}\")\n",
    "\n",
    "    # Prompt user to choose a folder\n",
    "    while True:\n",
    "        choice = input(\"Enter the number of the scan you want to choose: \")\n",
    "        if choice.isdigit() and 1 <= int(choice) <= len(scan_folders):\n",
    "            chosen_folder = scan_folders[int(choice) - 1]\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid input. Please enter a valid number.\")\n",
    "    print(\"Selected\",chosen_folder)\n",
    "    # Return the path to the chosen folder\n",
    "    return os.path.join(data_folder, chosen_folder), chosen_folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_similarity_transform(ref,points):\n",
    "    '''\n",
    "    \n",
    "    ref = np.vstack([landmarks_im['x'],landmarks_im['y']]).T\n",
    "    match = point_stream.data    \n",
    "    cor = np.vstack([match['x'],match['y']]).T\n",
    "    \n",
    "    M = estimate_similarity_transform(ref, cor)\n",
    "    \n",
    "    Joao Couto - wfield (2020)\n",
    "    '''\n",
    "    from skimage.transform import SimilarityTransform\n",
    "    M = SimilarityTransform()\n",
    "    M.estimate(ref,points)\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atlas_from_landmarks_file(landmarks_file=None,\n",
    "                              reference='dorsal_cortex',\n",
    "                              dims = [540,640],\n",
    "                              do_transform = None):\n",
    "    '''\n",
    "    Load the atlas regions, and area names and brain mask\n",
    "\n",
    "    Joao Couto - wfield 2020\n",
    "    '''\n",
    "    lmarks = load_allen_landmarks(landmarks_file)\n",
    "    ccf_regions,proj,brain_outline = allen_load_reference('dorsal_cortex')\n",
    "    # transform the regions into the image\n",
    "    if not 'transform' in lmarks.keys():\n",
    "        lmarks['transform'] = None\n",
    "    transform = lmarks['transform']\n",
    "    if not do_transform:\n",
    "        transform = None\n",
    "    nccf_regions = allen_transform_regions(transform,\n",
    "                                           ccf_regions,\n",
    "                                           resolution=lmarks['resolution'],\n",
    "                                           bregma_offset=lmarks['bregma_offset'])\n",
    "    nbrain_outline = apply_affine_to_points(brain_outline[:,0]/lmarks['resolution'] + lmarks['bregma_offset'][0],\n",
    "                                            brain_outline[:,1]/lmarks['resolution'] + lmarks['bregma_offset'][1],\n",
    "                                            transform)\n",
    "\n",
    "\n",
    "    atlas,areanames = allen_regions_to_atlas(nccf_regions, dims)\n",
    "    brain_mask = contour_to_mask(*nbrain_outline, dims = dims)\n",
    "    return atlas, areanames, brain_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'annotation_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-8f65354c59ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mallen_load_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreference_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotation_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotation_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     '''\n\u001b[1;32m      3\u001b[0m \u001b[0mLoad\u001b[0m \u001b[0mallen\u001b[0m \u001b[0mareas\u001b[0m \u001b[0mto\u001b[0m \u001b[0muse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mExample\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'annotation_dir' is not defined"
     ]
    }
   ],
   "source": [
    "def allen_load_reference(reference_name, annotation_dir):\n",
    "    '''\n",
    "Load allen areas to use as reference.\n",
    "\n",
    "Example:\n",
    "    ccf_regions,proj,brain_outline = allen_load_reference('dorsal_cortex')\n",
    "\n",
    "    Joao Couto - wfield, 2020\n",
    "    '''\n",
    "    if annotation_dir == wfield_dir:\n",
    "        # then it is the reference folder, download if not there\n",
    "        if not os.path.exists(pjoin(\n",
    "                annotation_dir,\n",
    "                '{0}_ccf_labels.json'.format(reference_name))):\n",
    "            from .utils import _create_wfield_folder\n",
    "            _create_wfield_folder()\n",
    "    from pandas import read_json\n",
    "    ccf_regions = read_json(pjoin(\n",
    "        annotation_dir,'{0}_ccf_labels.json'.format(reference_name)))\n",
    "    proj = np.load(pjoin(annotation_dir,\n",
    "                         '{0}_projection.npy'.format(reference_name),))\n",
    "    brain_outline = np.load(pjoin(annotation_dir,\n",
    "                                  '{0}_outline.npy'.format(reference_name)))\n",
    "    return ccf_regions,proj,brain_outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contour_to_mask(x,y,dims,extent = None,n_up_samples = 2000):\n",
    "    '''\n",
    "    Create a mask from a contour\n",
    "    \n",
    "    Usage:    \n",
    "        H = contour_to_mask(x,y,dims,extent = None,n_up_samples = 2000)\n",
    "    \n",
    "    Joao Couto - wfield, 2020        \n",
    "    '''\n",
    "\n",
    "    H = contour_to_im(x=x, y=y, \n",
    "                      dims = dims,\n",
    "                      extent = extent,\n",
    "                      n_up_samples = n_up_samples)    \n",
    "    # fix border cases\n",
    "    #if np.sum(H[0,:]):\n",
    "    #    H[0,:] = np.uint8(1)\n",
    "    #if np.sum(H[-1,:]):\n",
    "    #    H[-1,:] = np.uint8(1)\n",
    "    #if np.sum(H[:,0]):\n",
    "    #    H[:,0] = np.uint8(1)\n",
    "    #if np.sum(H[:,-1]):\n",
    "    #    H[:,-1] = np.uint8(1)\n",
    "    from scipy.ndimage import morphology\n",
    "    H = morphology.binary_dilation(H)\n",
    "    H = morphology.binary_fill_holes(H)\n",
    "    H = morphology.binary_erosion(H)\n",
    "    H[0,:] = 0\n",
    "    H[-1,:] = 0\n",
    "    H[:,0] = 0\n",
    "    H[:,-1] = 0\n",
    "    return H.astype(bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allen_regions_to_atlas(ccf_regions,dims,\n",
    "                           sides = ['left','right'],\n",
    "                           fillnan = False):\n",
    "    ''' \n",
    "    Atlas as called in locaNMF; it is the masks of allen areas.\n",
    "    This function returns also the names of the areas\n",
    "\n",
    "    Joao Couto - wfield 2020\n",
    "    '''\n",
    "    atlas = np.zeros(dims,dtype = np.float32)\n",
    "    if fillnan:\n",
    "        atlas.fill(np.nan)\n",
    "    areanames = []\n",
    "    for ireg,r in ccf_regions.iterrows():\n",
    "        for iside,side in enumerate(sides):\n",
    "            mask = contour_to_mask(\n",
    "                r[side+'_x'],r[side+'_y'],\n",
    "                dims = dims)\n",
    "            factor = 1\n",
    "            if iside==1:\n",
    "                factor = -1\n",
    "            atlas[mask==1] = factor*(ireg+1)\n",
    "            areanames.append([factor*(ireg+1),r['acronym']+'_'+side])\n",
    "    return atlas,areanames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_affine_to_points(x,y,M):\n",
    "    '''\n",
    "    Apply an affine transform to a set of contours or (x,y) points.\n",
    "\n",
    "    x,y = apply_affine_to_points(x, y, tranform)\n",
    "\n",
    "    Joao Couto - wfield (2020)\n",
    "    '''\n",
    "    if M is None:\n",
    "        nM = np.identity(3,dtype = np.float32)\n",
    "    else:\n",
    "        nM = M.params\n",
    "    xy = np.vstack([x,y,np.ones_like(y)])\n",
    "    res = (nM @ xy).T\n",
    "    return res[:,0],res[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allen_transform_from_landmarks(landmarks_im,match):\n",
    "    '''\n",
    "    Compute the similarity transform from annotated landmarks. \n",
    "    \n",
    "    transform = allen_transform_from_landmarks(landmarks_im,match)\n",
    "    \n",
    "    '''\n",
    "    ref = np.vstack([landmarks_im['x'],landmarks_im['y']]).T\n",
    "    cor = np.vstack([match['x'],match['y']]).T\n",
    "    return estimate_similarity_transform(ref, cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im_apply_transform(im,M,dims = None):\n",
    "    '''\n",
    "    Applies an affine transform M to an image.\n",
    "    nim = im_apply_transform(im,M)\n",
    "\n",
    "    Joao Couto - wfield, 2020\n",
    "    '''\n",
    "    if issparse(im):\n",
    "        # then reshape before\n",
    "        if dims is None:\n",
    "            raise ValueError('Provide dims when warping sparse matrices.')\n",
    "        shape = im.shape\n",
    "        tmp  = np.asarray(im.todense()).reshape(dims)\n",
    "        tmp = warp(tmp,M,\n",
    "                   order = 1,\n",
    "                   mode='constant',\n",
    "                   cval = 0,\n",
    "                   clip = True,\n",
    "                   preserve_range = True)\n",
    "        return csr_matrix(tmp.reshape(shape))\n",
    "    else:    \n",
    "        return warp(im,M,\n",
    "                    order = 1,\n",
    "                    mode='constant',\n",
    "                    cval = 0,\n",
    "                    clip=True,\n",
    "                    preserve_range=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im_apply_affine(im,transform):\n",
    "    W,H = im.shape\n",
    "    M = transform.params[:2,:]\n",
    "    return cv2.warpAffine(im, M, (H, W),cv2.WARP_INVERSE_MAP)\n",
    "def get_U_atlas(U,M):\n",
    "    U = U.copy()\n",
    "    U[:,0,:] = 1e-10\n",
    "    U[0,:,:] = 1e-10\n",
    "    U[-1,:,:] = 1e-10\n",
    "    U[:,-1,:] = 1e-10\n",
    "\n",
    "    # transpose U\n",
    "    return np.stack(runpar(im_apply_affine, U.transpose([2,0,1]),\n",
    "                           transform = M)).transpose([1,2,0]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_allen_landmarks(filename, reference = 'dorsal_cortex'):\n",
    "    '''\n",
    "    lmarks = load_allen_landmarks(filename, reference = 'dorsal_cortex'):\n",
    "    \n",
    "    Loads an allen landmark file (json) and returns the transform objects if present.\n",
    "    Joao Couto - wfield 2020\n",
    "    '''\n",
    "    if filename is None:\n",
    "        filename = pjoin(annotation_dir,reference + '_landmarks.json')\n",
    "    if not os.path.exists(filename):\n",
    "        if '.wfield' in filename:\n",
    "            from .utils import _create_wfield_folder\n",
    "            _create_wfield_folder()\n",
    "        else:\n",
    "            raise(OSError('Could not find the reference file {0}.'.format(filename)))\n",
    "\n",
    "    with open(filename,'r') as fd:\n",
    "        import json\n",
    "        lmarks = json.load(fd)\n",
    "    for k in ['landmarks_im','landmarks','landmarks_match']:\n",
    "        if k in lmarks.keys():\n",
    "            from pandas import DataFrame\n",
    "            lmarks[k] = DataFrame(lmarks[k])[['x','y','name','color']]\n",
    "    if 'transform' in lmarks.keys():\n",
    "        if not 'transform_type' in lmarks.keys():\n",
    "            lmarks['transform_type'] = 'euclidian'\n",
    "        if lmarks['transform_type'] == 'affine':\n",
    "            from skimage.transform import AffineTransform\n",
    "            lmarks['transform'] = AffineTransform(\n",
    "                np.array(lmarks['transform']))\n",
    "        else: # use similarity\n",
    "            from skimage.transform import SimilarityTransform\n",
    "            lmarks['transform'] = SimilarityTransform(\n",
    "                np.array(lmarks['transform']))\n",
    "        if 'transform_inverse' in lmarks.keys():\n",
    "            if lmarks['transform_type'] == 'affine':\n",
    "                from skimage.transform import AffineTransform\n",
    "                lmarks['transform_inverse'] = AffineTransform(\n",
    "                    np.array(lmarks['transform_inverse']))\n",
    "            else: # use similarity\n",
    "                from skimage.transform import SimilarityTransform\n",
    "                lmarks['transform_inverse'] = SimilarityTransform(\n",
    "                    np.array(lmarks['transform_inverse']))\n",
    "    return lmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_locaNMF(U,V,atlas,brain_mask,\n",
    "                    minrank = 1, # rank = how many components per brain region.\n",
    "                    maxrank = 10, #Set maxrank to around 10 for regular dataset.\n",
    "                    min_pixels = 100, # minimum number of pixels in Allen map for it to be considered a brain region\n",
    "                    loc_thresh = 70, # Localization threshold, i.e. percentage of area restricted to be inside the 'atlas boundary'\n",
    "                    r2_thresh = 0.99, # Fraction of variance in the data to capture with LocaNMF\n",
    "                    nonnegative_temporal = False, # Do you want nonnegative temporal components? The data itself should also be nonnegative in this case.\n",
    "                    maxiter_lambda = 300,\n",
    "                    device = 'auto',\n",
    "                    verbose = [True, False, False]):\n",
    "    '''\n",
    "This function runs locaNMF from wfield analysis outputs.\n",
    "It uses the original package for LocaNMF, written by Ian Kinsella and Shreya Saxena\n",
    "Reference: \n",
    "    Saxena S, Kinsella I, Musall S, Kim SH, Meszaros J, et al. (2020) \n",
    "    Localized semi-nonnegative matrix factorization (LocaNMF) of widefield calcium imaging data. \n",
    "    PLOS Computational Biology 16(4): e1007791. https://doi.org/10.1371/journal.pcbi.1007791\n",
    "\n",
    "Usage:\n",
    "    \n",
    "    A,C,regions = compute_locaNMF(U,V,atlas,brain_mask,\n",
    "                    minrank = 1, \n",
    "                    maxrank = 10, \n",
    "                    min_pixels = 100,\n",
    "                    loc_thresh = 70, \n",
    "                    r2_thresh = 0.99,\n",
    "                    device = 'cuda')\n",
    "    \n",
    "    Joao Couto - wfield, 2023\n",
    "    '''\n",
    "    try:\n",
    "        from locanmf import LocaNMF\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        raise(OSError(\"This analysis requires the locaNMF package.\"))\n",
    "    \n",
    "    import torch\n",
    "    if device == 'auto':\n",
    "        if torch.cuda.is_available():\n",
    "            device = 'cuda'\n",
    "        else:\n",
    "            print('torch could not find a cuda capable GPU, using the CPU (slower).')\n",
    "            device = 'cpu'\n",
    "            \n",
    "    rank_range = (minrank, maxrank, 1)\n",
    "    if nonnegative_temporal:\n",
    "        r = V.T\n",
    "    else:\n",
    "        q, r = np.linalg.qr(V.T)\n",
    "    video_mats = (np.copy(U[brain_mask]), r.T)\n",
    "    del U\n",
    "\n",
    "    region_mats = LocaNMF.extract_region_metadata(brain_mask, atlas, min_size=min_pixels)\n",
    "    region_metadata = LocaNMF.RegionMetadata(region_mats[0].shape[0],\n",
    "                                               region_mats[0].shape[1:],\n",
    "                                               device=device)\n",
    "\n",
    "    region_metadata.set(torch.from_numpy(region_mats[0].astype(np.uint8)),\n",
    "                        torch.from_numpy(region_mats[1]),\n",
    "                        torch.from_numpy(region_mats[2].astype(np.int64)))\n",
    "\n",
    "    # Do SVD\n",
    "    if device=='cuda': torch.cuda.synchronize()\n",
    "    region_videos = LocaNMF.factor_region_videos(video_mats,\n",
    "                                                   region_mats[0],\n",
    "                                                   rank_range[1],\n",
    "                                                   device=device)\n",
    "    if device=='cuda': torch.cuda.synchronize()\n",
    "    low_rank_video = LocaNMF.LowRankVideo(\n",
    "        (int(np.sum(brain_mask)),) + video_mats[1].shape, device=device)\n",
    "    low_rank_video.set(torch.from_numpy(video_mats[0].T),\n",
    "                       torch.from_numpy(video_mats[1]))\n",
    "    if device=='cuda': torch.cuda.synchronize()\n",
    "    locanmf_comps = LocaNMF.rank_linesearch(low_rank_video,\n",
    "                                            region_metadata,\n",
    "                                            region_videos,\n",
    "                                            maxiter_rank = maxrank-minrank+1,\n",
    "                                            maxiter_lambda = maxiter_lambda, \n",
    "                                            maxiter_hals = 20,\n",
    "                                            lambda_step = 1.35,\n",
    "                                            lambda_init = 1e-6, \n",
    "                                            loc_thresh = loc_thresh,\n",
    "                                            r2_thresh = r2_thresh,\n",
    "                                            rank_range = rank_range,\n",
    "                                            nnt = nonnegative_temporal,\n",
    "                                            verbose = verbose,\n",
    "                                            sample_prop = (1,1),\n",
    "                                            device = device)\n",
    "    if device=='cuda': torch.cuda.synchronize()\n",
    "    # Get LocaNMF spatial and temporal components\n",
    "    A = locanmf_comps.spatial.data.cpu().numpy().T\n",
    "    A_reshape = np.zeros((brain_mask.shape[0],brain_mask.shape[1],A.shape[1])); A_reshape.fill(np.nan)\n",
    "    A_reshape[brain_mask,:] = A\n",
    "\n",
    "    if nonnegative_temporal:\n",
    "        C = locanmf_comps.temporal.data.cpu().numpy()\n",
    "    else:\n",
    "        C = np.matmul(q,locanmf_comps.temporal.data.cpu().numpy().T).T\n",
    "\n",
    "    regions = region_metadata.labels.data[locanmf_comps.regions.data].cpu().numpy()\n",
    "\n",
    "    if device=='cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return A_reshape,C,regions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_scans(data_folder, keyword):\n",
    "    # Find folders containing the keyword\n",
    "    scan_folders = [folder for folder in os.listdir(data_folder) if os.path.isdir(os.path.join(data_folder, folder)) and keyword in folder]\n",
    "\n",
    "    # Print list of found folders\n",
    "    print(\"Folders containing '{}' keyword:\")\n",
    "    for i, folder in enumerate(scan_folders):\n",
    "        print(f\"{i + 1}. {folder}\")\n",
    "\n",
    "    # Prompt user to choose a folder\n",
    "    while True:\n",
    "        choice = input(\"Enter the number of the scan you want to choose: \")\n",
    "        if choice.isdigit() and 1 <= int(choice) <= len(scan_folders):\n",
    "            chosen_folder = scan_folders[int(choice) - 1]\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid input. Please enter a valid number.\")\n",
    "    print(\"Selected\",chosen_folder)\n",
    "    # Return the path to the chosen folder\n",
    "    return os.path.join(data_folder, chosen_folder), chosen_folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmap_dat(filename,\n",
    "             mode = 'r',\n",
    "             nframes = None,\n",
    "             shape = None,\n",
    "             dtype='uint16'):\n",
    "    '''\n",
    "    Loads frames from a binary file as a memory map.\n",
    "    This is useful when the data does not fit to memory.\n",
    "    \n",
    "    Inputs:\n",
    "        filename (str)       : fileformat convention, file ends in _NCHANNELS_H_W_DTYPE.dat\n",
    "        mode (str)           : memory map access mode (default 'r')\n",
    "                'r'   | Open existing file for reading only.\n",
    "                'r+'  | Open existing file for reading and writing.                 \n",
    "        nframes (int)        : number of frames to read (default is None: the entire file)\n",
    "        offset (int)         : offset frame number (default 0)\n",
    "        shape (list|tuple)   : dimensions (NCHANNELS, HEIGHT, WIDTH) default is None\n",
    "        dtype (str)          : datatype (default uint16) \n",
    "    Returns:\n",
    "        A memory mapped  array with size (NFRAMES,NCHANNELS, HEIGHT, WIDTH).\n",
    "\n",
    "    Example:\n",
    "        dat = mmap_dat(filename)\n",
    "    '''\n",
    "    \n",
    "    if not os.path.isfile(filename):\n",
    "        raise OSError('File {0} not found.'.format(filename))\n",
    "    if shape is None or dtype is None: # try to get it from the filename\n",
    "        dtype,shape,_ = _parse_binary_fname(filename,\n",
    "                                            shape = shape,\n",
    "                                            dtype = dtype)\n",
    "    if type(dtype) is str:\n",
    "        dt = np.dtype(dtype)\n",
    "    else:\n",
    "        dt = dtype\n",
    "    if nframes is None:\n",
    "        # Get the number of samples from the file size\n",
    "        nframes = int(os.path.getsize(filename)/(np.prod(shape)*dt.itemsize))\n",
    "    dt = np.dtype(dtype)\n",
    "    return np.memmap(filename,\n",
    "                     mode=mode,\n",
    "                     dtype=dt,\n",
    "                     shape = (int(nframes),*shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join(a, *p):\n",
    "    \"\"\"Join two or more pathname components, inserting '/' as needed.\n",
    "    If any component is an absolute path, all previous path components\n",
    "    will be discarded.  An empty last part will result in a path that\n",
    "    ends with a separator.\"\"\"\n",
    "    a = os.fspath(a)\n",
    "    sep = _get_sep(a)\n",
    "    path = a\n",
    "    try:\n",
    "        if not p:\n",
    "            path[:0] + sep  #23780: Ensure compatible data type even if p is null.\n",
    "        for b in map(os.fspath, p):\n",
    "            if b.startswith(sep):\n",
    "                path = b\n",
    "            elif not path or path.endswith(sep):\n",
    "                path += b\n",
    "            else:\n",
    "                path += sep + b\n",
    "    except (TypeError, AttributeError, BytesWarning):\n",
    "        genericpath._check_arg_types('join', a, *p)\n",
    "        raise\n",
    "    return path\n",
    "\n",
    "def glob(pathname, *, root_dir=None, dir_fd=None, recursive=False,\n",
    "        include_hidden=False):\n",
    "    \"\"\"Return a list of paths matching a pathname pattern.\n",
    "\n",
    "    The pattern may contain simple shell-style wildcards a la\n",
    "    fnmatch. Unlike fnmatch, filenames starting with a\n",
    "    dot are special cases that are not matched by '*' and '?'\n",
    "    patterns by default.\n",
    "\n",
    "    If `include_hidden` is true, the patterns '*', '?', '**'  will match hidden\n",
    "    directories.\n",
    "\n",
    "    If `recursive` is true, the pattern '**' will match any files and\n",
    "    zero or more directories and subdirectories.\n",
    "    \"\"\"\n",
    "    return list(iglob(pathname, root_dir=root_dir, dir_fd=dir_fd, recursive=recursive,\n",
    "                      include_hidden=include_hidden))\n",
    "def _get_sep(path):\n",
    "    if isinstance(path, bytes):\n",
    "        return b'/'\n",
    "    else:\n",
    "        return '/'\n",
    "\n",
    "def iglob(pathname, *, root_dir=None, dir_fd=None, recursive=False,\n",
    "          include_hidden=False):\n",
    "    \"\"\"Return an iterator which yields the paths matching a pathname pattern.\n",
    "\n",
    "    The pattern may contain simple shell-style wildcards a la\n",
    "    fnmatch. However, unlike fnmatch, filenames starting with a\n",
    "    dot are special cases that are not matched by '*' and '?'\n",
    "    patterns.\n",
    "\n",
    "    If recursive is true, the pattern '**' will match any files and\n",
    "    zero or more directories and subdirectories.\n",
    "    \"\"\"\n",
    "    sys.audit(\"glob.glob\", pathname, recursive)\n",
    "    sys.audit(\"glob.glob/2\", pathname, recursive, root_dir, dir_fd)\n",
    "    if root_dir is not None:\n",
    "        root_dir = os.fspath(root_dir)\n",
    "    else:\n",
    "        root_dir = pathname[:0]\n",
    "    it = _iglob(pathname, root_dir, dir_fd, recursive, False,\n",
    "                include_hidden=include_hidden)\n",
    "    if not pathname or recursive and _isrecursive(pathname[:2]):\n",
    "        try:\n",
    "            s = next(it)  # skip empty string\n",
    "            if s:\n",
    "                it = itertools.chain((s,), it)\n",
    "        except StopIteration:\n",
    "            pass\n",
    "    return it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_binary_fname(fname,lastidx=None, dtype = 'uint16', shape = None, sep = '_'):\n",
    "    '''\n",
    "    Gets the data type and the shape from the filename \n",
    "    This is a helper function to use in load_dat.\n",
    "    \n",
    "    out = _parse_binary_fname(fname)\n",
    "    \n",
    "    With out default to: \n",
    "        out = dict(dtype=dtype, shape = shape, fnum = None)\n",
    "    '''\n",
    "    fn = os.path.splitext(os.path.basename(fname))[0]\n",
    "    fnsplit = fn.split(sep)\n",
    "    fnum = None\n",
    "    if lastidx is None:\n",
    "        # find the datatype first (that is the first dtype string from last)\n",
    "        lastidx = -1\n",
    "        idx = np.where([not f.isnumeric() for f in fnsplit])[0]\n",
    "        for i in idx[::-1]:\n",
    "            try:\n",
    "                dtype = np.dtype(fnsplit[i])\n",
    "                lastidx = i\n",
    "            except TypeError:\n",
    "                pass\n",
    "    if dtype is None:\n",
    "        dtype = np.dtype(fnsplit[lastidx])\n",
    "    # further split in those before and after lastidx\n",
    "    before = [f for f in fnsplit[:lastidx] if f.isdigit()]\n",
    "    after = [f for f in fnsplit[lastidx:] if f.isdigit()]\n",
    "    if shape is None:\n",
    "        # then the shape are the last 3\n",
    "        shape = [int(t) for t in before[-3:]]\n",
    "    if len(after)>0:\n",
    "        fnum = [int(t) for t in after]\n",
    "    return dtype,shape,fnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders containing '{}' keyword:\n",
      "1. Habituation_AA_WEZ-8950_2024-04-16_scan9FNN1Y64_sess9FNN0LFP\n",
      "2. Habituation_AA_WEZ-8948_2024-04-17_scan9FNNP1N7_sess9FNNO3Z1\n",
      "3. Oddball_AA_ROS-1706_2024-03-12_scan9FN2BCOS_sess9FN2ANVG\n",
      "4. AA_ROS-1688_2024_01_27_scan000WQU9_sess000EAEIO\n",
      "5. Habituation_AA_WEZ-8950_2024-04-18_scan9FNO8CLT_sess9FNO8CLT\n",
      "6. Habituation_AA_WEZ-8950_2024-04-17_scan9FNNO3Z1_sess9FNNO3Z1\n",
      "7. AA_ROS-1706_2024-03-12_scan9FN2ANVG_sess9FN2ANVG\n",
      "8. Habituation_AA_WEZ-8950_2024-04-16_scan9FNN1YXK_sess9FNN1YXK\n",
      "9. Habituation_AA_WEZ-8948_2024-04-18_scan9FNO99ZE_sess9FNO8CLT\n",
      "10. AA_ROS-1688_2024_01_27_scan000EAEIO_sess000EAEIO\n",
      "11. Habituation_AA_WEZ-8950_2024-04-16_scan9FNN2FX7_sess9FNN1YXK\n",
      "12. Habituation_AA_WEZ-8948_2024-04-16_scan9FNN1M1R_sess9FNN0LFP\n",
      "Selected AA_ROS-1706_2024-03-12_scan9FN2ANVG_sess9FN2ANVG\n"
     ]
    }
   ],
   "source": [
    "data_folder = r'/datajoint-data/data/aeltona/'\n",
    "# tif_file_path = pjoin(data_folder, 'scan9FN2ANVG_Oddball_AA_ROS-1706_2025_MMStack_Default.ome.tif')\n",
    "# localdisk = r'C:\\datatemp'\n",
    "localdisk, scan_idx = list_scans(data_folder,\"AA\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_path = os.path.join(localdisk,'scan9FN2ANVG_Oddball_AA_ROS-1706_600_600_2_uint16.dat')\n",
    "dat = mmap_dat(dat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'allen_load_reference' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-c54b13d2e80d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# from .allen import atlas_from_landmarks_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matlas_from_landmarks_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlmarksfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Using the mask from the landmarks file for decomposition.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a01a1557713f>\u001b[0m in \u001b[0;36matlas_from_landmarks_file\u001b[0;34m(landmarks_file, reference, dims, do_transform)\u001b[0m\n\u001b[1;32m      9\u001b[0m     '''\n\u001b[1;32m     10\u001b[0m     \u001b[0mlmarks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_allen_landmarks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlandmarks_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mccf_regions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbrain_outline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mallen_load_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dorsal_cortex'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m# transform the regions into the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m'transform'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlmarks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'allen_load_reference' is not defined"
     ]
    }
   ],
   "source": [
    "lmarksfile = os.path.join(localdisk,'ccf_transform_landmarks.json')\n",
    "lmarks = load_allen_landmarks(os.path.join(localdisk,'ccf_transform_landmarks.json'))\n",
    "\n",
    "if len(lmarks):\n",
    "    mask = np.zeros(dat.shape[-2::],dtype=bool)\n",
    "    # from .allen import atlas_from_landmarks_file\n",
    "    _, _, mask = atlas_from_landmarks_file(lmarksfile,dims = mask.shape, do_transform=True)\n",
    "    print('Using the mask from the landmarks file for decomposition.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "locanmf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
